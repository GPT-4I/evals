{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"/workspaces/evals/dataset\"\n",
    "system_input = \"\"\n",
    "\n",
    "def battle(system_input1, user_input1, completion1, system_input2, user_input2, completion2):\n",
    "\n",
    "    battle_path = os.path.join(os.getcwd(), \"registry\", \"data\", \"battle\")\n",
    "    os.makedirs(battle_path, exist_ok=True)\n",
    "    battle_path = os.path.join(battle_path, \"samples.jsonl\")\n",
    "\n",
    "    # Data\n",
    "    input1 = ([{\"role\":\"system\",\"content\":system_input1},{\"role\":\"user\",\"content\":user_input1}])\n",
    "    input2 = ([{\"role\":\"system\",\"content\":system_input2},{\"role\":\"user\",\"content\":user_input2}])\n",
    "    dataset = [{\"input1\": input1, \"completion1\": completion1, \"input2\": input2, \"completion2\":completion2}]\n",
    "    pd.DataFrame(dataset).to_json(battle_path, orient=\"records\", lines=True)\n",
    "    \n",
    "    # Run eval\n",
    "    !oaieval gpt-3.5-turbo battle --record_path /workspaces/evals/evallogs/logs\n",
    "    \n",
    "    # Report\n",
    "    with open(\"/workspaces/evals/evallogs/logs\", \"r\") as f:\n",
    "        events_df = pd.read_json(f, lines=True)\n",
    "\n",
    "    record_path = \"/workspaces/evals/evallogs/eval\"\n",
    "    os.makedirs(record_path, exist_ok=True)\n",
    "    events_df.to_json(os.path.join(record_path, \"events\"), lines=True, orient=\"records\")\n",
    "    events_df[\"data\"].to_json(os.path.join(record_path, \"data\"), lines=True, orient=\"records\")\n",
    "    data_df = pd.read_json(os.path.join(record_path, \"data\"), lines=True)\n",
    "    for row in data_df[0]:\n",
    "        s = str(row)\n",
    "        s = s.replace(\"[\",\"\",-1)\n",
    "        s = s.replace(\"]\",\"\",-1)\n",
    "        if s.startswith(\"{'choice':\"):\n",
    "            s = s.split(\"'\")[3]\n",
    "            choice = s\n",
    "        if s.startswith(\"{'prompt': {'role': 'user', 'content': 'You are comparing two responses to the following two instructions.\"):\n",
    "            sampled = s.split(\"\\\\n\\\\nReasoning:\\'}, \\'sampled\\': \")[1].replace(\"\\\\'\",\"'\").replace(\"\\\\n\",\"\\n\")\n",
    "    \n",
    "    data = {'Choice': choice, 'Instruction1': user_input1, 'Response1': completion1, 'Instruction2': user_input2, 'Response2': completion2, 'Sampled': sampled}\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def test_match(test_dataset):\n",
    "    \n",
    "    data = []\n",
    "    for t in range(0,len(test_dataset)):\n",
    "        data.append({\"input\":[{\"role\":\"system\",\"content\":\"\"},{\"role\":\"user\",\"content\":test_dataset[t][0]}], \"ideal\":test_dataset[t][2]})\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Paths. Assuming this notebook is in evals/evals\n",
    "\n",
    "    test_match_path = os.path.join(os.getcwd(), \"registry\", \"data\", \"match\")\n",
    "    os.makedirs(test_match_path, exist_ok=True)\n",
    "    test_match_path = os.path.join(test_match_path, \"samples.jsonl\")\n",
    "\n",
    "    df.to_json(test_match_path, orient=\"records\", lines=True)\n",
    "    \n",
    "    !oaieval gpt-3.5-turbo match --record_path /workspaces/evals/evallogs/logs\n",
    "    \n",
    "    events = \"/workspaces/evals/evallogs/logs\"\n",
    "    record_path = os.path.join(\"/workspaces/evals/evallogs/eval\")\n",
    "    with open(events, \"r\") as f:\n",
    "        events_df = pd.read_json(f, lines=True)\n",
    "    \n",
    "    os.makedirs(record_path, exist_ok=True)\n",
    "    events_df.to_json(os.path.join(record_path, \"events\"), lines=True, orient=\"records\")\n",
    "    events_df[\"data\"].to_json(os.path.join(record_path, \"data\"), lines=True, orient=\"records\")\n",
    "    data_df = pd.read_json(os.path.join(record_path, \"data\"), lines=True)\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "def content_to_list(content, dataset):\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    return dataset[dataset_df[0].to_list().index(content)]\n",
    "\n",
    "def append_response(content, response, dataset):\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    index = dataset_df[0].to_list().index(content)\n",
    "    dataset[index][2].append(response)\n",
    "    return len(dataset[index][2])\n",
    "dataset_df = pd.read_json(os.path.join(dataset_path, \"dataset\"), lines=True)\n",
    "test_dataset_df = pd.read_json(os.path.join(dataset_path, \"test_dataset\"), lines=True)\n",
    "best_response_df = pd.read_json(os.path.join(dataset_path, \"best_response\"), lines=True)\n",
    "best_response_input_df = pd.read_json(os.path.join(dataset_path, \"best_response_input\"), lines=True)\n",
    "battles_df = pd.read_json(os.path.join(dataset_path, \"battles\"), lines=True)\n",
    "n_responses_df = pd.read_json(os.path.join(dataset_path, \"n_responses\"), lines=True)\n",
    "test_match_dataset_df = pd.read_json(os.path.join(dataset_path, \"test_match_dataset\"), lines=True)\n",
    "\n",
    "dataset = dataset_df.values.tolist()\n",
    "test_dataset = test_dataset_df.values.tolist()\n",
    "best_response = best_response_df.values.tolist()\n",
    "best_response_input = best_response_input_df.values.tolist()\n",
    "n_responses = n_responses_df.values.tolist()\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "\n",
    "    test_match_data = test_match(test_dataset)\n",
    "    test_match_dataset_df = pd.concat([test_match_dataset_df, test_match_data])\n",
    "\n",
    "    same_best_response = True\n",
    "    for i in range(3,len(test_match_data[0]),2):\n",
    "        if test_match_data[0][i][\"correct\"] == False:\n",
    "            sampled = test_match_data[0][i][\"sampled\"]\n",
    "            if sampled not in dataset_df[0].to_list():\n",
    "                content = test_match_data[0][i-1][\"prompt\"][1][\"content\"]\n",
    "                dataset.append([sampled,content,[]])\n",
    "                dataset_df = pd.DataFrame(dataset)\n",
    "                n = append_response(content, sampled, dataset)\n",
    "                n_responses.append([n])\n",
    "\n",
    "                data = battle(system_input,content,sampled,system_input,best_response_input[-1][0],best_response[-1][0]) \n",
    "                battles_df = pd.concat([battles_df,pd.DataFrame(data, index=[0])], ignore_index=True)\n",
    "\n",
    "                if data[\"Choice\"] == \"Yes\":\n",
    "                    best_response_input.append([content])\n",
    "                    best_response.append([sampled])\n",
    "                    test_dataset = [content_to_list(content,dataset)]\n",
    "                    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "                    same_best_response = False\n",
    "\n",
    "    if same_best_response:\n",
    "        test_dataset0 = test_dataset.copy()\n",
    "        for t in test_dataset0:\n",
    "            if t[0] != \"\":\n",
    "                if t[1] not in test_dataset_df[0].to_list():\n",
    "                    test_dataset.append(content_to_list(t[1],dataset))\n",
    "                    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "            l = content_to_list(t[0], dataset)\n",
    "            for response in l[2]:\n",
    "                if response not in test_dataset_df[0].to_list():\n",
    "                    test_dataset.append(content_to_list(response,dataset))\n",
    "                    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "    best_response_df = pd.DataFrame(best_response)\n",
    "    best_response_input_df = pd.DataFrame(best_response_input)\n",
    "    n_responses_df = pd.DataFrame(n_responses)  \n",
    "    \n",
    "    dataset_df.to_json(os.path.join(dataset_path, \"dataset\"), lines=True, orient=\"records\")\n",
    "    test_dataset_df.to_json(os.path.join(dataset_path, \"test_dataset\"), lines=True, orient=\"records\")\n",
    "    best_response_df.to_json(os.path.join(dataset_path, \"best_response\"), lines=True, orient=\"records\")\n",
    "    best_response_input_df.to_json(os.path.join(dataset_path, \"best_response_input\"), lines=True, orient=\"records\")\n",
    "    n_responses_df.to_json(os.path.join(dataset_path, \"n_responses\"), lines=True, orient=\"records\")\n",
    "\n",
    "    pd.DataFrame(dataset_df[0]).to_json(os.path.join(dataset_path, \"dataset0\"), lines=True, orient=\"records\")\n",
    "    pd.DataFrame(dataset_df[1]).to_json(os.path.join(dataset_path, \"dataset1\"), lines=True, orient=\"records\")\n",
    "\n",
    "    battles_df.to_json(os.path.join(dataset_path, \"battles\"), lines=True, orient=\"records\")\n",
    "    test_match_dataset_df.to_json(os.path.join(dataset_path, \"test_match_dataset\"), lines=True, orient=\"records\")\n",
    "\n",
    "    responses = []\n",
    "    for responses_list in dataset_df[2]:\n",
    "        responses.append(len(responses_list))\n",
    "        pd.Series(responses)\n",
    "    responses_df = pd.DataFrame({\"len\":responses, \"responses\": dataset_df[2]})\n",
    "    responses_df.to_json(os.path.join(dataset_path, \"responses\"), lines=True, orient=\"records\")\n",
    "\n",
    "    best_responses_battles_df = battles_df[battles_df[\"Choice\"]==\"Yes\"]\n",
    "    pd.DataFrame(best_responses_battles_df[\"Sampled\"]).to_json(os.path.join(dataset_path, \"best_responses_battles_sampled\"), lines=True, orient=\"records\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
