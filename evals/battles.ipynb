{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1473/3990185712.py:90: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  battles_df = pd.read_json(os.path.join(dataset_path, \"battles\"), lines=True)\n",
      "/tmp/ipykernel_1473/3990185712.py:90: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  battles_df = pd.read_json(os.path.join(dataset_path, \"battles\"), lines=True)\n",
      "/tmp/ipykernel_1473/3990185712.py:90: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  battles_df = pd.read_json(os.path.join(dataset_path, \"battles\"), lines=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-27 13:14:40,834] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:14:41,663] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:14:41,665] [oaieval.py:138] \u001b[1;35mRun started: 230727131441RK5WWBCQ\u001b[0m\n",
      "[2023-07-27 13:14:41,666] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-27 13:14:41,666] [eval.py:33] Evaluating 30 samples\n",
      "[2023-07-27 13:14:41,673] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|███████████████████████████████████████████| 30/30 [00:10<00:00,  2.82it/s]\n",
      "[2023-07-27 13:14:52,331] [record.py:341] Final report: {'accuracy': 0.6333333333333333, 'boostrap_std': 0.08947997913872502}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:14:52,331] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:14:52,331] [oaieval.py:179] accuracy: 0.6333333333333333\n",
      "[2023-07-27 13:14:52,331] [oaieval.py:179] boostrap_std: 0.08947997913872502\n",
      "[2023-07-27 13:14:52,338] [record.py:330] Logged 60 rows of events to /workspaces/evals/evallogs/logs: insert_time=5.432ms\n",
      "[2023-07-27 13:14:53,715] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:14:54,475] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:14:54,476] [oaieval.py:138] \u001b[1;35mRun started: 230727131454RKDKSWQO\u001b[0m\n",
      "[2023-07-27 13:14:54,478] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:14:54,507] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:14:54,507] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:14:54,507] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:14:54,513] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.60s/it]\n",
      "[2023-07-27 13:15:02,120] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:15:02,121] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:15:02,121] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:15:02,121] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:15:02,123] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=1.341ms\n",
      "[2023-07-27 13:15:03,312] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:15:04,090] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:15:04,091] [oaieval.py:138] \u001b[1;35mRun started: 230727131504DAXZCL5B\u001b[0m\n",
      "[2023-07-27 13:15:04,093] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:15:04,125] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:15:04,125] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:15:04,125] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:15:04,132] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.49s/it]\n",
      "[2023-07-27 13:15:11,633] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:15:11,633] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:15:11,633] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:15:11,633] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:15:11,635] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.586ms\n",
      "[2023-07-27 13:15:12,902] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:15:13,576] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:15:13,577] [oaieval.py:138] \u001b[1;35mRun started: 230727131513MC6CGQ6G\u001b[0m\n",
      "[2023-07-27 13:15:13,578] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:15:13,606] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:15:13,607] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:15:13,607] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:15:13,613] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.08s/it]\n",
      "[2023-07-27 13:15:20,700] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:15:20,701] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:15:20,701] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:15:20,701] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:15:20,702] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.608ms\n",
      "[2023-07-27 13:15:22,049] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:15:22,782] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:15:22,784] [oaieval.py:138] \u001b[1;35mRun started: 230727131522K5DU56F2\u001b[0m\n",
      "[2023-07-27 13:15:22,785] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:15:22,817] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:15:22,817] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:15:22,818] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:15:22,823] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.67s/it]\n",
      "[2023-07-27 13:15:30,492] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:15:30,492] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:15:30,492] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:15:30,492] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:15:30,494] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.932ms\n",
      "[2023-07-27 13:15:31,868] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:15:32,565] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:15:32,566] [oaieval.py:138] \u001b[1;35mRun started: 23072713153252SFTYLM\u001b[0m\n",
      "[2023-07-27 13:15:32,567] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:15:32,595] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:15:32,595] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:15:32,595] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:15:32,600] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.30s/it]\n",
      "[2023-07-27 13:15:39,909] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:15:39,909] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:15:39,909] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:15:39,909] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:15:39,911] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.766ms\n",
      "[2023-07-27 13:15:41,144] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:15:42,037] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:15:42,038] [oaieval.py:138] \u001b[1;35mRun started: 230727131542FOYU2RNS\u001b[0m\n",
      "[2023-07-27 13:15:42,039] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:15:42,067] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:15:42,067] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:15:42,067] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:15:42,072] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.67s/it]\n",
      "[2023-07-27 13:15:48,751] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:15:48,751] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:15:48,751] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:15:48,751] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:15:48,753] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.727ms\n",
      "[2023-07-27 13:15:50,038] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:15:50,767] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:15:50,769] [oaieval.py:138] \u001b[1;35mRun started: 230727131550LBGF7J5W\u001b[0m\n",
      "[2023-07-27 13:15:50,770] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:15:50,798] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:15:50,798] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:15:50,799] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:15:50,803] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.15s/it]\n",
      "[2023-07-27 13:15:57,964] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:15:57,964] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:15:57,964] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:15:57,964] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:15:57,965] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.488ms\n",
      "[2023-07-27 13:15:59,184] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:15:59,978] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:15:59,980] [oaieval.py:138] \u001b[1;35mRun started: 230727131559UZSEK4WD\u001b[0m\n",
      "[2023-07-27 13:15:59,982] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:16:00,009] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:16:00,009] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:16:00,010] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:16:00,015] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.00s/it]\n",
      "[2023-07-27 13:16:08,024] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:16:08,024] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:16:08,024] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:16:08,024] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:16:08,025] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.565ms\n",
      "[2023-07-27 13:16:09,248] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:16:10,081] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:16:10,083] [oaieval.py:138] \u001b[1;35mRun started: 2307271316104XNN2RRD\u001b[0m\n",
      "[2023-07-27 13:16:10,085] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:16:10,120] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:16:10,120] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:16:10,121] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:16:10,126] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.36s/it]\n",
      "[2023-07-27 13:16:17,490] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:16:17,490] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:16:17,490] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:16:17,490] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:16:17,492] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.551ms\n",
      "[2023-07-27 13:16:18,731] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:16:19,450] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:16:19,451] [oaieval.py:138] \u001b[1;35mRun started: 230727131619ARPBKYGN\u001b[0m\n",
      "[2023-07-27 13:16:19,452] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-27 13:16:19,453] [eval.py:33] Evaluating 44 samples\n",
      "[2023-07-27 13:16:19,459] [eval.py:139] Running in threaded mode with 10 threads!\n",
      " 98%|██████████████████████████████████████████ | 43/44 [00:14<00:00,  3.13it/s][2023-07-27 13:16:51,832] [_common.py:105] Backing off openai_chat_completion_create_retrying(...) for 0.2s (openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.)\n",
      "100%|███████████████████████████████████████████| 44/44 [00:35<00:00,  1.24it/s]\n",
      "[2023-07-27 13:16:55,118] [record.py:341] Final report: {'accuracy': 0.5227272727272727, 'boostrap_std': 0.07496357242076077}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:16:55,119] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:16:55,119] [oaieval.py:179] accuracy: 0.5227272727272727\n",
      "[2023-07-27 13:16:55,119] [oaieval.py:179] boostrap_std: 0.07496357242076077\n",
      "[2023-07-27 13:16:55,128] [record.py:330] Logged 88 rows of events to /workspaces/evals/evallogs/logs: insert_time=8.019ms\n",
      "[2023-07-27 13:16:56,413] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:16:57,163] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:16:57,164] [oaieval.py:138] \u001b[1;35mRun started: 230727131657CNUFN3K6\u001b[0m\n",
      "[2023-07-27 13:16:57,165] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:16:57,194] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:16:57,194] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:16:57,194] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:16:57,198] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.48s/it]\n",
      "[2023-07-27 13:17:07,682] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:17:07,682] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:17:07,682] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:17:07,682] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:17:07,684] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=1.031ms\n",
      "[2023-07-27 13:17:09,106] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:17:09,888] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:17:09,890] [oaieval.py:138] \u001b[1;35mRun started: 230727131709OEP2G6LH\u001b[0m\n",
      "[2023-07-27 13:17:09,891] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:17:09,923] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:17:09,923] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:17:09,923] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:17:09,927] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.23s/it]\n",
      "[2023-07-27 13:17:17,162] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:17:17,162] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:17:17,162] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:17:17,162] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:17:17,164] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.620ms\n",
      "[2023-07-27 13:17:18,412] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:17:19,195] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:17:19,197] [oaieval.py:138] \u001b[1;35mRun started: 2307271317196ZWR4SNA\u001b[0m\n",
      "[2023-07-27 13:17:19,199] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:17:19,238] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:17:19,238] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:17:19,239] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:17:19,243] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.81s/it]\n",
      "[2023-07-27 13:17:31,055] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:17:31,056] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:17:31,056] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:17:31,056] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:17:31,057] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.848ms\n",
      "[2023-07-27 13:17:32,317] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:17:33,004] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:17:33,005] [oaieval.py:138] \u001b[1;35mRun started: 230727131733SDJ73NX7\u001b[0m\n",
      "[2023-07-27 13:17:33,007] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:17:33,038] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:17:33,038] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:17:33,040] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:17:33,044] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s][2023-07-27 13:18:06,980] [_common.py:105] Backing off openai_chat_completion_create_retrying(...) for 1.5s (openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:40<00:00, 40.44s/it]\n",
      "[2023-07-27 13:18:13,484] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:18:13,484] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:18:13,484] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:18:13,485] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:18:13,487] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.522ms\n",
      "[2023-07-27 13:18:14,727] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:18:15,937] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:18:15,938] [oaieval.py:138] \u001b[1;35mRun started: 230727131815HSU7RC74\u001b[0m\n",
      "[2023-07-27 13:18:15,940] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:18:15,967] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:18:15,967] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:18:15,967] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:18:15,973] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:12<00:00, 12.09s/it]\n",
      "[2023-07-27 13:18:28,062] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:18:28,062] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:18:28,062] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:18:28,062] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:18:28,063] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.621ms\n",
      "[2023-07-27 13:18:29,273] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:18:30,078] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:18:30,079] [oaieval.py:138] \u001b[1;35mRun started: 230727131830LKJUJS3G\u001b[0m\n",
      "[2023-07-27 13:18:30,081] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:18:30,113] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:18:30,113] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:18:30,114] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:18:30,119] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:09<00:00,  9.06s/it]\n",
      "[2023-07-27 13:18:39,187] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:18:39,187] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:18:39,187] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:18:39,187] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:18:39,189] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.671ms\n",
      "[2023-07-27 13:18:40,366] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:18:41,132] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:18:41,133] [oaieval.py:138] \u001b[1;35mRun started: 230727131841NJYWJLN3\u001b[0m\n",
      "[2023-07-27 13:18:41,135] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:18:41,164] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:18:41,164] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:18:41,165] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:18:41,170] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.62s/it]\n",
      "[2023-07-27 13:18:48,794] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:18:48,794] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:18:48,794] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:18:48,794] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:18:48,796] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.788ms\n",
      "[2023-07-27 13:18:49,994] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:18:50,664] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:18:50,665] [oaieval.py:138] \u001b[1;35mRun started: 2307271318506RWHSHJM\u001b[0m\n",
      "[2023-07-27 13:18:50,666] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:18:50,695] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:18:50,695] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:18:50,696] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:18:50,700] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.44s/it]\n",
      "[2023-07-27 13:18:59,147] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:18:59,147] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:18:59,147] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:18:59,147] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:18:59,149] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.583ms\n",
      "[2023-07-27 13:19:00,359] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:19:01,040] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:19:01,041] [oaieval.py:138] \u001b[1;35mRun started: 230727131901GFY5LZQ5\u001b[0m\n",
      "[2023-07-27 13:19:01,043] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:19:01,070] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:19:01,071] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:19:01,071] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:19:01,077] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.83s/it]\n",
      "[2023-07-27 13:19:09,911] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:19:09,911] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:19:09,911] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:19:09,911] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:19:09,913] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.608ms\n",
      "[2023-07-27 13:19:11,105] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:19:11,791] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:19:11,793] [oaieval.py:138] \u001b[1;35mRun started: 230727131911Q5XDGFCP\u001b[0m\n",
      "[2023-07-27 13:19:11,794] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:19:11,825] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:19:11,825] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:19:11,826] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:19:11,831] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.79s/it]\n",
      "[2023-07-27 13:19:19,622] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:19:19,622] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:19:19,622] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:19:19,622] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:19:19,623] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.673ms\n",
      "[2023-07-27 13:19:20,854] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:19:21,532] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:19:21,533] [oaieval.py:138] \u001b[1;35mRun started: 230727131921BELY6LXQ\u001b[0m\n",
      "[2023-07-27 13:19:21,535] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:19:21,562] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:19:21,562] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:19:21,562] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:19:21,569] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.26s/it]\n",
      "[2023-07-27 13:19:29,828] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:19:29,828] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:19:29,828] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:19:29,828] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:19:29,830] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.585ms\n",
      "[2023-07-27 13:19:31,067] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:19:32,052] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:19:32,053] [oaieval.py:138] \u001b[1;35mRun started: 230727131932ODHKFMOW\u001b[0m\n",
      "[2023-07-27 13:19:32,054] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 13:19:32,085] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 13:19:32,085] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 13:19:32,086] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 13:19:32,092] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.10s/it]\n",
      "[2023-07-27 13:19:40,191] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:19:40,191] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:19:40,192] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 13:19:40,192] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 13:19:40,193] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.684ms\n",
      "[2023-07-27 13:19:41,472] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 13:19:42,158] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 13:19:42,159] [oaieval.py:138] \u001b[1;35mRun started: 230727131942KH7425X7\u001b[0m\n",
      "[2023-07-27 13:19:42,161] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-27 13:19:42,161] [eval.py:33] Evaluating 65 samples\n",
      "[2023-07-27 13:19:42,166] [eval.py:139] Running in threaded mode with 10 threads!\n",
      " 75%|████████████████████████████████▍          | 49/65 [00:16<00:05,  3.14it/s][2023-07-27 13:19:59,777] [record.py:330] Logged 100 rows of events to /workspaces/evals/evallogs/logs: insert_time=9.223ms\n",
      "100%|███████████████████████████████████████████| 65/65 [00:22<00:00,  2.92it/s]\n",
      "[2023-07-27 13:20:04,457] [record.py:341] Final report: {'accuracy': 0.5846153846153846, 'boostrap_std': 0.06564675234731723}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 13:20:04,457] [oaieval.py:177] Final report:\n",
      "[2023-07-27 13:20:04,457] [oaieval.py:179] accuracy: 0.5846153846153846\n",
      "[2023-07-27 13:20:04,457] [oaieval.py:179] boostrap_std: 0.06564675234731723\n",
      "[2023-07-27 13:20:04,460] [record.py:330] Logged 30 rows of events to /workspaces/evals/evallogs/logs: insert_time=2.640ms\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'correct'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 108\u001b[0m\n\u001b[1;32m    106\u001b[0m same_best_response \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m,\u001b[39mlen\u001b[39m(test_match_data[\u001b[39m0\u001b[39m]),\u001b[39m2\u001b[39m):\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mif\u001b[39;00m test_match_data[\u001b[39m0\u001b[39;49m][i][\u001b[39m\"\u001b[39;49m\u001b[39mcorrect\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         sampled \u001b[39m=\u001b[39m test_match_data[\u001b[39m0\u001b[39m][i][\u001b[39m\"\u001b[39m\u001b[39msampled\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    110\u001b[0m         \u001b[39mif\u001b[39;00m sampled \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m dataset_df[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto_list():\n",
      "\u001b[0;31mKeyError\u001b[0m: 'correct'"
     ]
    }
   ],
   "source": [
    "runs = 4\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"/workspaces/evals/dataset\"\n",
    "system_input = None\n",
    "\n",
    "def battle(system_input1, user_input1, completion1, system_input2, user_input2, completion2):\n",
    "\n",
    "    battle_path = os.path.join(os.getcwd(), \"registry\", \"data\", \"battle\")\n",
    "    os.makedirs(battle_path, exist_ok=True)\n",
    "    battle_path = os.path.join(battle_path, \"samples.jsonl\")\n",
    "\n",
    "    # Data\n",
    "    if system_input1 = None:\n",
    "        input1 = ([{\"role\":\"user\",\"content\":user_input1}])\n",
    "    else:\n",
    "        input1 = ([{\"role\":\"system\",\"content\":system_input1},{\"role\":\"user\",\"content\":user_input1}])\n",
    "\n",
    "    if system_input2 = None:\n",
    "        input2 = ([{\"role\":\"user\",\"content\":user_input2}])\n",
    "    else:\n",
    "        input2 = ([{\"role\":\"system\",\"content\":system_input2},{\"role\":\"user\",\"content\":user_input2}])\n",
    "\n",
    "    dataset = [{\"input1\": input1, \"completion1\": completion1, \"input2\": input2, \"completion2\":completion2}]\n",
    "    pd.DataFrame(dataset).to_json(battle_path, orient=\"records\", lines=True)\n",
    "    \n",
    "    # Run eval\n",
    "    !oaieval gpt-3.5-turbo battle --record_path /workspaces/evals/evallogs/logs\n",
    "    \n",
    "    # Report\n",
    "    with open(\"/workspaces/evals/evallogs/logs\", \"r\") as f:\n",
    "        events_df = pd.read_json(f, lines=True)\n",
    "\n",
    "    record_path = \"/workspaces/evals/evallogs/eval\"\n",
    "    os.makedirs(record_path, exist_ok=True)\n",
    "    events_df.to_json(os.path.join(record_path, \"events\"), lines=True, orient=\"records\")\n",
    "    events_df[\"data\"].to_json(os.path.join(record_path, \"data\"), lines=True, orient=\"records\")\n",
    "    data_df = pd.read_json(os.path.join(record_path, \"data\"), lines=True)\n",
    "    for row in data_df[0]:\n",
    "        s = str(row)\n",
    "        s = s.replace(\"[\",\"\",-1)\n",
    "        s = s.replace(\"]\",\"\",-1)\n",
    "        if s.startswith(\"{'choice':\"):\n",
    "            s = s.split(\"'\")[3]\n",
    "            choice = s\n",
    "        if s.startswith(\"{'prompt': {'role': 'user', 'content': 'You are comparing two responses to the following two instructions.\"):\n",
    "            sampled = s.split(\"\\\\n\\\\nReasoning:\\'}, \\'sampled\\': \")[1].replace(\"\\\\'\",\"'\").replace(\"\\\\n\",\"\\n\")\n",
    "    \n",
    "    data = {'Choice': choice, 'Instruction1': user_input1, 'Response1': completion1, 'Instruction2': user_input2, 'Response2': completion2, 'Sampled': sampled}\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def test_match(test_dataset):\n",
    "    \n",
    "    data = []\n",
    "    for t in range(0,len(test_dataset)):\n",
    "        data.append({\"input\":[{\"role\":\"system\",\"content\":\"\"},{\"role\":\"user\",\"content\":test_dataset[t][0]}], \"ideal\":test_dataset[t][2]})\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Paths. Assuming this notebook is in evals/evals\n",
    "\n",
    "    test_match_path = os.path.join(os.getcwd(), \"registry\", \"data\", \"match\")\n",
    "    os.makedirs(test_match_path, exist_ok=True)\n",
    "    test_match_path = os.path.join(test_match_path, \"samples.jsonl\")\n",
    "\n",
    "    df.to_json(test_match_path, orient=\"records\", lines=True)\n",
    "    \n",
    "    !oaieval gpt-3.5-turbo match --record_path /workspaces/evals/evallogs/logs\n",
    "    \n",
    "    events = \"/workspaces/evals/evallogs/logs\"\n",
    "    record_path = os.path.join(\"/workspaces/evals/evallogs/eval\")\n",
    "    with open(events, \"r\") as f:\n",
    "        events_df = pd.read_json(f, lines=True)\n",
    "    \n",
    "    os.makedirs(record_path, exist_ok=True)\n",
    "    events_df.to_json(os.path.join(record_path, \"events\"), lines=True, orient=\"records\")\n",
    "    events_df[\"data\"].to_json(os.path.join(record_path, \"data\"), lines=True, orient=\"records\")\n",
    "    data_df = pd.read_json(os.path.join(record_path, \"data\"), lines=True)\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "def content_to_list(content, dataset):\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    return dataset[dataset_df[0].to_list().index(content)]\n",
    "\n",
    "def append_response(content, response, dataset):\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    index = dataset_df[0].to_list().index(content)\n",
    "    dataset[index][2].append(response)\n",
    "    return len(dataset[index][2])\n",
    "dataset_df = pd.read_json(os.path.join(dataset_path, \"dataset\"), lines=True)\n",
    "test_dataset_df = pd.read_json(os.path.join(dataset_path, \"test_dataset\"), lines=True)\n",
    "best_response_df = pd.read_json(os.path.join(dataset_path, \"best_response\"), lines=True)\n",
    "best_response_input_df = pd.read_json(os.path.join(dataset_path, \"best_response_input\"), lines=True)\n",
    "battles_df = pd.read_json(os.path.join(dataset_path, \"battles\"), lines=True)\n",
    "n_responses_df = pd.read_json(os.path.join(dataset_path, \"n_responses\"), lines=True)\n",
    "test_match_dataset_df = pd.read_json(os.path.join(dataset_path, \"test_match_dataset\"), lines=True)\n",
    "\n",
    "dataset = dataset_df.values.tolist()\n",
    "test_dataset = test_dataset_df.values.tolist()\n",
    "best_response = best_response_df.values.tolist()\n",
    "best_response_input = best_response_input_df.values.tolist()\n",
    "n_responses = n_responses_df.values.tolist()\n",
    "\n",
    "\n",
    "for i in range(runs):\n",
    "\n",
    "    test_match_data = test_match(test_dataset)\n",
    "    test_match_dataset_df = pd.concat([test_match_dataset_df, test_match_data])\n",
    "\n",
    "    same_best_response = True\n",
    "    for i in range(3,len(test_match_data[0]),2):\n",
    "        if test_match_data[0][i][\"correct\"] == False:\n",
    "            sampled = test_match_data[0][i][\"sampled\"]\n",
    "            if sampled not in dataset_df[0].to_list():\n",
    "                content = test_match_data[0][i-1][\"prompt\"][1][\"content\"]\n",
    "                dataset.append([sampled,content,[]])\n",
    "                dataset_df = pd.DataFrame(dataset)\n",
    "                n = append_response(content, sampled, dataset)\n",
    "                n_responses.append([n])\n",
    "\n",
    "                data = battle(system_input,content,sampled,system_input,best_response_input[-1][0],best_response[-1][0]) \n",
    "                battles_df = pd.concat([battles_df,pd.DataFrame(data, index=[0])], ignore_index=True)\n",
    "\n",
    "                if data[\"Choice\"] == \"Yes\":\n",
    "                    best_response_input.append([content])\n",
    "                    best_response.append([sampled])\n",
    "                    test_dataset = [content_to_list(content,dataset)]\n",
    "                    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "                    same_best_response = False\n",
    "\n",
    "    if same_best_response:\n",
    "        test_dataset0 = test_dataset.copy()\n",
    "        for t in test_dataset0:\n",
    "            if t[0] != \"\":\n",
    "                if t[1] not in test_dataset_df[0].to_list():\n",
    "                    test_dataset.append(content_to_list(t[1],dataset))\n",
    "                    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "            l = content_to_list(t[0], dataset)\n",
    "            for response in l[2]:\n",
    "                if response not in test_dataset_df[0].to_list():\n",
    "                    test_dataset.append(content_to_list(response,dataset))\n",
    "                    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "    best_response_df = pd.DataFrame(best_response)\n",
    "    best_response_input_df = pd.DataFrame(best_response_input)\n",
    "    n_responses_df = pd.DataFrame(n_responses)  \n",
    "    \n",
    "    dataset_df.to_json(os.path.join(dataset_path, \"dataset\"), lines=True, orient=\"records\")\n",
    "    test_dataset_df.to_json(os.path.join(dataset_path, \"test_dataset\"), lines=True, orient=\"records\")\n",
    "    best_response_df.to_json(os.path.join(dataset_path, \"best_response\"), lines=True, orient=\"records\")\n",
    "    best_response_input_df.to_json(os.path.join(dataset_path, \"best_response_input\"), lines=True, orient=\"records\")\n",
    "    n_responses_df.to_json(os.path.join(dataset_path, \"n_responses\"), lines=True, orient=\"records\")\n",
    "\n",
    "    pd.DataFrame(dataset_df[0]).to_json(os.path.join(dataset_path, \"dataset0\"), lines=True, orient=\"records\")\n",
    "    pd.DataFrame(dataset_df[1]).to_json(os.path.join(dataset_path, \"dataset1\"), lines=True, orient=\"records\")\n",
    "\n",
    "    battles_df.to_json(os.path.join(dataset_path, \"battles\"), lines=True, orient=\"records\")\n",
    "    test_match_dataset_df.to_json(os.path.join(dataset_path, \"test_match_dataset\"), lines=True, orient=\"records\")\n",
    "\n",
    "    responses = []\n",
    "    for responses_list in dataset_df[2]:\n",
    "        responses.append(len(responses_list))\n",
    "        pd.Series(responses)\n",
    "    responses_df = pd.DataFrame({\"len\":responses, \"responses\": dataset_df[2]})\n",
    "    responses_df.to_json(os.path.join(dataset_path, \"responses\"), lines=True, orient=\"records\")\n",
    "\n",
    "    best_responses_battles_df = battles_df[battles_df[\"Choice\"]==\"Yes\"]\n",
    "    pd.DataFrame(best_responses_battles_df[\"Sampled\"]).to_json(os.path.join(dataset_path, \"best_responses_battles_sampled\"), lines=True, orient=\"records\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'prompt': [{'role': 'system', 'content': ''},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'correct': True, 'expected': 'Thank you so mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'prompt': [{'role': 'system', 'content': ''},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'correct': False, 'expected': [], 'picked': N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>{'correct': True, 'expected': ['Thank you so m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>{'prompt': [{'role': 'system', 'content': ''},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>{'correct': False, 'expected': [], 'picked': N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>{'prompt': [{'role': 'system', 'content': ''},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>{'correct': False, 'expected': [], 'picked': N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0                                                 None\n",
       "1    {'prompt': [{'role': 'system', 'content': ''},...\n",
       "2    {'correct': True, 'expected': 'Thank you so mu...\n",
       "3    {'prompt': [{'role': 'system', 'content': ''},...\n",
       "4    {'correct': False, 'expected': [], 'picked': N...\n",
       "..                                                 ...\n",
       "127  {'correct': True, 'expected': ['Thank you so m...\n",
       "128  {'prompt': [{'role': 'system', 'content': ''},...\n",
       "129  {'correct': False, 'expected': [], 'picked': N...\n",
       "130  {'prompt': [{'role': 'system', 'content': ''},...\n",
       "131  {'correct': False, 'expected': [], 'picked': N...\n",
       "\n",
       "[132 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_match_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
