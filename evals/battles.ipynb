{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "#import openai\n",
    "\n",
    "system_input1 = \"\"\n",
    "user_input1 = \"\"\n",
    "\n",
    "system_input2 = \"\"\n",
    "user_input2 = \"\"\n",
    "\n",
    "# Data\n",
    "\n",
    "best_response = [\"\"]\n",
    "best_response_input = [user_input1]\n",
    "test_dataset = [[user_input1,None,[]]]\n",
    "dataset = [test_dataset[0]]\n",
    "battles = []\n",
    "\n",
    "# Paths. Assuming this notebook is in evals/evals\n",
    "\n",
    "registry_path = os.path.join(os.getcwd(), \"registry\")\n",
    "dataset_path = (\"/workspaces/evals/dataset\")\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "os.makedirs(\"/workspaces/evals/evallogs\", exist_ok=True)\n",
    "\n",
    "# Registry yaml\n",
    "\n",
    "registry_yaml = {}\n",
    "registry_yaml[\"battle\"] = {\n",
    "    \"id\": \"battle.test.v1\",\n",
    "    \"metrics\": [\"accuracy\"]\n",
    "}\n",
    "registry_yaml[\"battle.test.v1\"] = {\n",
    "    \"class\": \"evals.elsuite.modelgraded.classify:ModelBasedClassify\",\n",
    "    \"args\": {\n",
    "        \"samples_jsonl\": \"battle/samples.jsonl\",\n",
    "        \"eval_type\": \"cot_classify\",\n",
    "        \"modelgraded_spec\": \"battle\",\n",
    "        \"sample_kwargs\":\n",
    "            {\"temperature\": 0.0}  }\n",
    "}\n",
    "\n",
    "\n",
    "with open(os.path.join(registry_path, \"evals\", \"battle.yaml\"), \"w\") as f:\n",
    "    yaml.dump(registry_yaml, f)\n",
    "\n",
    "\n",
    "registry_yaml = {}\n",
    "registry_yaml[\"match\"] = {\n",
    "    \"id\": \"match.test.v1\",\n",
    "    \"metrics\": [\"accuracy\"]\n",
    "}\n",
    "registry_yaml[\"match.test.v1\"] = {\n",
    "    \"class\": \"evals.elsuite.basic.sample_match:Match\",\n",
    "    \"args\": {\n",
    "        \"samples_jsonl\": \"match/samples.jsonl\",\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "with open(os.path.join(registry_path, \"evals\", \"match.yaml\"), \"w\") as f:\n",
    "    yaml.dump(registry_yaml, f)\n",
    "\n",
    "def battle(system_input1, user_input1, completion1, system_input2, user_input2, completion2):\n",
    "\n",
    "    # Data\n",
    "\n",
    "    input1 = ([{\"role\":\"system\",\"content\":system_input1},{\"role\":\"user\",\"content\":user_input1}])\n",
    "    input2 = ([{\"role\":\"system\",\"content\":system_input2},{\"role\":\"user\",\"content\":user_input2}])\n",
    "\n",
    "    dataset = [{\"input1\": input1, \"completion1\": completion1, \"input2\": input2, \"completion2\":completion2}]\n",
    "   \n",
    "    # Paths. Assuming this notebook is in evals/evals\n",
    "\n",
    "    battle_path = os.path.join(os.getcwd(), \"registry\", \"data\", \"battle\")\n",
    "    os.makedirs(battle_path, exist_ok=True)\n",
    "    battle_path = os.path.join(battle_path, \"samples.jsonl\")\n",
    "\n",
    "    df = pd.DataFrame(dataset)\n",
    "    df.to_json(battle_path, orient=\"records\", lines=True)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    message_content = \"You are comparing two responses to the following two instructions.\"\n",
    "    \n",
    "    message_content += \"\\n\\n[Instruction 1]\\nUser: \"\n",
    "    message_content += user_input1\n",
    "    message_content += \"\\n\\n[Response 1]\\n\"\n",
    "    message_content += completion1\n",
    "\n",
    "    message_content += \"\\n\\n[Instruction 2]\\nUser: \"\n",
    "    message_content += user_input2\n",
    "    message_content += \"\\n\\n[Response 2]\\n\"\n",
    "    message_content += completion2 \n",
    "\n",
    "    message_content += \"\\n\\n\\nIs the first response better than the second? You must provide one answer based on your subjective view.\\n\\nFirst, write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Then print only a single choice from \\\"Yes\\\" or \\\"No\\\" (without quotes or punctuation) on its own line corresponding to the correct answer. At the end, repeat just the answer by itself on a new line.\\n\\nReasoning:\"\n",
    "\n",
    "    openai.api_key = \"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\"\n",
    "    data = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "        {\"role\": \"user\", \"content\": message_content},\n",
    "        ]\n",
    "    )\n",
    "    \"\"\"\n",
    "    !oaieval gpt-3.5-turbo battle --record_path /workspaces/evals/evallogs/logs\n",
    "    \n",
    "    events = \"/workspaces/evals/evallogs/logs\"\n",
    "    record_path = os.path.join(\"/workspaces/evals/evallogs/eval\")\n",
    "    with open(events, \"r\") as f:\n",
    "        events_df = pd.read_json(f, lines=True)\n",
    "\n",
    "    os.makedirs(record_path, exist_ok=True)\n",
    "    events_df.to_json(os.path.join(record_path, \"events\"), lines=True, orient=\"records\")\n",
    "    events_df[\"data\"].to_json(os.path.join(record_path, \"data\"), lines=True, orient=\"records\")\n",
    "    data_df = pd.read_json(os.path.join(record_path, \"data\"), lines=True)\n",
    "    for row in data_df[0]:\n",
    "        s = str(row)\n",
    "        s = s.replace(\"[\",\"\",-1)\n",
    "        s = s.replace(\"]\",\"\",-1)\n",
    "        if s.startswith(\"{'choice':\"):\n",
    "            s = s.split(\"'\")[3]\n",
    "            choice = s\n",
    "        if s.startswith(\"{'prompt': {'role': 'user', 'content': 'You are comparing two responses to the following two instructions.\"):\n",
    "            #s = s.split(\"\\\\n\\\\nInstruction 1\\\\n\",)[1]\n",
    "            #instruction1 = s.split(\"\\\\n\\\\nResponse 1\\\\n\")[0].replace(\"\\\\'\",\"'\").replace(\"\\\\n\",\"\\n\")\n",
    "            #s = s.split(\"\\\\nResponse 1\\\\n\",)[1]\n",
    "            #response1 = s.split(\"\\\\n\\\\nInstruction 2\\\\n\")[0].replace(\"\\\\'\",\"'\").replace(\"\\\\n\",\"\\n\")\n",
    "            #s = s.split(\"\\\\n\\\\nInstruction 2\\\\n\",)[1]\n",
    "            #instruction2 = s.split(\"\\\\n\\\\nResponse 2\\\\n\")[0].replace(\"\\\\'\",\"'\").replace(\"\\\\n\",\"\\n\")\n",
    "            #s = s.split(\"\\\\nResponse 2\\\\n\",)[1]\n",
    "            #response2 = s.split(\"\\\\n\\\\n\\\\nWhich of the following responses is better?\")[0].replace(\"\\\\'\",\"'\").replace(\"\\\\n\",\"\\n\")\n",
    "            sampled = s.split(\"\\\\n\\\\nReasoning:\\'}, \\'sampled\\': \")[1].replace(\"\\\\'\",\"'\").replace(\"\\\\n\",\"\\n\")\n",
    "    \n",
    "    #content = data[\"choices\"][\"message\"][\"content\"]\n",
    "    #choice = content.split(\"\\\\n\")[-1]\n",
    "    data = {'Choice': choice, 'Instruction1': user_input1, 'Response1': completion1, 'Instruction2': user_input2, 'Response2': completion2, 'Sampled': sampled}\n",
    "    \n",
    "    return data\n",
    "def test_match(test_dataset):\n",
    "    data = []\n",
    "    for t in range(0,len(test_dataset)):\n",
    "        data.append({\"input\":[{\"role\":\"system\",\"content\":\"\"},{\"role\":\"user\",\"content\":test_dataset[t][0]}], \"ideal\":test_dataset[t][2]})\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Paths. Assuming this notebook is in evals/evals\n",
    "\n",
    "    test_match_path = os.path.join(os.getcwd(), \"registry\", \"data\", \"match\")\n",
    "    os.makedirs(test_match_path, exist_ok=True)\n",
    "    test_match_path = os.path.join(test_match_path, \"samples.jsonl\")\n",
    "\n",
    "    df.to_json(test_match_path, orient=\"records\", lines=True)\n",
    "    \n",
    "    !oaieval gpt-3.5-turbo match --record_path /workspaces/evals/evallogs/logs\n",
    "    \n",
    "    events = \"/workspaces/evals/evallogs/logs\"\n",
    "    record_path = os.path.join(\"/workspaces/evals/evallogs/eval\")\n",
    "    with open(events, \"r\") as f:\n",
    "        events_df = pd.read_json(f, lines=True)\n",
    "    \n",
    "    os.makedirs(record_path, exist_ok=True)\n",
    "    events_df.to_json(os.path.join(record_path, \"events\"), lines=True, orient=\"records\")\n",
    "    events_df[\"data\"].to_json(os.path.join(record_path, \"data\"), lines=True, orient=\"records\")\n",
    "    data_df = pd.read_json(os.path.join(record_path, \"data\"), lines=True)\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "def content_to_list(content, dataset):\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    return dataset[dataset_df[0].to_list().index(content)]\n",
    "\n",
    "def append_response(content, response, dataset):\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    index = dataset_df[0].to_list().index(content)\n",
    "    dataset[index][2].append(response)\n",
    "    return None\n",
    "\n",
    "dataset_df = pd.DataFrame(dataset)\n",
    "test_dataset_df = pd.DataFrame(test_dataset)\n",
    "best_response_df = pd.DataFrame(best_response)\n",
    "battles_df = pd.DataFrame(battles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-24 12:27:34,003] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-24 12:27:34,735] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-24 12:27:34,736] [oaieval.py:138] \u001b[1;35mRun started: 2307241227345Q7TLLIU\u001b[0m\n",
      "[2023-07-24 12:27:34,737] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-24 12:27:34,737] [eval.py:33] Evaluating 2 samples\n",
      "[2023-07-24 12:27:34,742] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.19it/s]\n",
      "[2023-07-24 12:27:36,434] [record.py:341] Final report: {'accuracy': 0.5, 'boostrap_std': 0.49993599590347565}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-24 12:27:36,434] [oaieval.py:177] Final report:\n",
      "[2023-07-24 12:27:36,434] [oaieval.py:179] accuracy: 0.5\n",
      "[2023-07-24 12:27:36,434] [oaieval.py:179] boostrap_std: 0.49993599590347565\n",
      "[2023-07-24 12:27:36,435] [record.py:330] Logged 4 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.541ms\n",
      "[2023-07-24 12:27:37,634] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-24 12:27:38,377] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-24 12:27:38,379] [oaieval.py:138] \u001b[1;35mRun started: 230724122738L53PRFLL\u001b[0m\n",
      "[2023-07-24 12:27:38,380] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-24 12:27:38,405] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-24 12:27:38,405] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-24 12:27:38,406] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-24 12:27:38,411] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.66s/it]\n",
      "[2023-07-24 12:27:43,073] [record.py:341] Final report: {'counts/Yes': 1, 'score': 1.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-24 12:27:43,073] [oaieval.py:177] Final report:\n",
      "[2023-07-24 12:27:43,073] [oaieval.py:179] counts/Yes: 1\n",
      "[2023-07-24 12:27:43,073] [oaieval.py:179] score: 1.0\n",
      "[2023-07-24 12:27:43,075] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.436ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_df.to_json(os.path.join(dataset_path, \"dataset\"), lines=True, orient=\"records\")\n",
    "test_dataset_df.to_json(os.path.join(dataset_path, \"test_dataset\"), lines=True, orient=\"records\")\n",
    "best_response_df.to_json(os.path.join(dataset_path, \"best_response\"), lines=True, orient=\"records\")\n",
    "battles_df.to_json(os.path.join(dataset_path, \"battles\"), lines=True, orient=\"records\")\n",
    "\n",
    "test_match_data = test_match(test_dataset)\n",
    "\n",
    "same_best_response = True\n",
    "for column in test_match_data.columns:\n",
    "    if test_match_data[column][3][\"correct\"] == False:\n",
    "        content = test_match_data[column][2][\"prompt\"][1][\"content\"]\n",
    "        content_list = content_to_list(content,dataset)\n",
    "        if content_list[1] is not None:\n",
    "            parent_content_list = content_to_list(content_list[1],dataset)\n",
    "        sampled = test_match_data[column][3][\"sampled\"]\n",
    "        data = battle(system_input1,content,sampled,system_input2,best_response_input[-1],best_response[-1]) \n",
    "        dataset.append([sampled,content,[]])\n",
    "        battles.append(data)\n",
    "        append_response(content, sampled, dataset)\n",
    "        if data[\"Choice\"] == \"Yes\":\n",
    "            best_response_input.append(content)\n",
    "            best_response.append(sampled)\n",
    "            test_dataset = [content_list]\n",
    "            same_best_response = False\n",
    "\n",
    "if same_best_response:\n",
    "    for t in test_dataset:\n",
    "        if t[1] is not None:\n",
    "            if t[1] not in test_dataset_df[0].to_list():\n",
    "                test_dataset.append(content_to_list(t[1],dataset))\n",
    "        l = content_to_list(t[0], dataset)\n",
    "        for response in l[2]:\n",
    "            if response not in test_dataset_df[0].to_list():\n",
    "                test_dataset.append(content_to_list(response,dataset))\n",
    "\n",
    "dataset_df = pd.DataFrame(dataset)\n",
    "test_dataset_df = pd.DataFrame(test_dataset)\n",
    "best_response_df = pd.DataFrame(best_response)\n",
    "battles_df = pd.DataFrame(battles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['',\n",
       "  None,\n",
       "  [\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\"]],\n",
       " [\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  '',\n",
       "  [\"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\"]],\n",
       " [\"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\",\n",
       "  \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  []]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       " '',\n",
       " []]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df[0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       " \"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  '',\n",
       "  [\"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\"]],\n",
       " ['',\n",
       "  None,\n",
       "  [\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\"]],\n",
       " [\"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\",\n",
       "  \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  []]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'prompt': [{'role': 'system', 'content': ''},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'correct': False, 'expected': [], 'picked': N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                                               None\n",
       "1                                               None\n",
       "2  {'prompt': [{'role': 'system', 'content': ''},...\n",
       "3  {'correct': False, 'expected': [], 'picked': N..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_match_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=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\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "test_match() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(dataset_test)):\n\u001b[1;32m      3\u001b[0m     test_match_data \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m     test_match_data\u001b[39m.\u001b[39mappend(test_match(dataset_test[d][\u001b[39m0\u001b[39;49m],dataset_test[d][\u001b[39m2\u001b[39;49m]))\n\u001b[1;32m      5\u001b[0m dataset_test\u001b[39m.\u001b[39mappend(dataset(dataset[:][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mindex(dataset_test[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m])))\n",
      "\u001b[0;31mTypeError\u001b[0m: test_match() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "for d in range(0,len(dataset_test)):\n",
    "    test_match_data = []\n",
    "    test_match_data.append(test_match(dataset_test[d][0],dataset_test[d][2]))\n",
    "dataset_test.append(dataset(dataset[:][0].index(dataset_test[0][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.append([])\n",
    "for u in user_content:\n",
    "    data = battle(system_input1,u,system_input2,best_response_input)\n",
    "    if data[\"Choice\"] == \"Yes\":\n",
    "        best_response_input = data[\"Instruction1\"]\n",
    "        best_response.append(data[\"Response1\"]) \n",
    "    dataset[u].append(data)\n",
    "user_content.append(data[\"Response1\"])\n",
    "df = pd.DataFrame(dataset)\n",
    "user_content_df = pd.DataFrame(user_content)\n",
    "df.to_json(os.path.join(os.getcwd(), \"battles.jsonl\"), orient=\"records\", lines=True)\n",
    "user_content_df.to_json(os.path.join(os.getcwd(), \"battles_user_content.jsonl\"), orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-shot prompt\n",
    "\n",
    "choice = int(df[\"Choice\"][0])-1\n",
    "if choice == 1:\n",
    "    input[0] = [\n",
    "        {\"role\": \"system\", \"content\": user_input[choice], \"name\": \"example_user\"},\n",
    "        {\"role\": \"system\", \"content\": df[df.columns[1+(2*choice)]][0], \"name\": \"example_assistant\"},\n",
    "        {\"role\":\"system\",\"content\":\"\"},\n",
    "        {\"role\":\"user\",\"content\":user_input[1]},\n",
    "        ]\n",
    "    dataset = [{\"input1\": input[0], \"input2\": input[1]}]\n",
    "    df = pd.DataFrame(dataset)\n",
    "    df.to_json(os.path.join(os.getcwd(), \"battles.jsonl\"), orient=\"records\", lines=True)\n",
    "    df.to_json(os.path.join(os.getcwd(), \"battles.jsonl\"), orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instruction(i):\n",
    "    if i.startswith(\"User: \"):\n",
    "        input = [{\"role\":\"system\",\"content\":\"\"},{\"role\":\"user\",\"content\":i.removeprefix(\"User: \")}]\n",
    "    else:\n",
    "        ins = i.split(\"\\nUser: \")\n",
    "        input = [{\"role\":\"system\",\"content\":ins[0]},{\"role\":\"user\",\"content\":ins[1]}]\n",
    "    return input\n",
    "meta = []\n",
    "for index, row in df.iterrows():\n",
    "    meta.append({\"input1\": instruction(row[0]), \"completion1\": row[1], \"input2\": instruction(row[2]), \"completion2\": row[3], \"choice\":\"\"})\n",
    "meta = pd.DataFrame(meta)\n",
    "\n",
    "row = 5\n",
    "meta.iloc[row][0],meta.iloc[row][1],meta.iloc[row][2],meta.iloc[row][3]\n",
    "\n",
    "meta.iloc[row][4] = \"1\"\n",
    "#meta.to_json(meta_path, orient=\"records\", lines=True)\n",
    "!oaieval gpt-3.5-turbo dataset-meta --record_path /workspaces/evals/evallogs/logs-meta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
