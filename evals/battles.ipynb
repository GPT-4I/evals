{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"/workspaces/evals/dataset\"\n",
    "system_input = \"\"\n",
    "\n",
    "def battle(system_input1, user_input1, completion1, system_input2, user_input2, completion2):\n",
    "\n",
    "    battle_path = os.path.join(os.getcwd(), \"registry\", \"data\", \"battle\")\n",
    "    os.makedirs(battle_path, exist_ok=True)\n",
    "    battle_path = os.path.join(battle_path, \"samples.jsonl\")\n",
    "\n",
    "    # Data\n",
    "    input1 = ([{\"role\":\"system\",\"content\":system_input1},{\"role\":\"user\",\"content\":user_input1}])\n",
    "    input2 = ([{\"role\":\"system\",\"content\":system_input2},{\"role\":\"user\",\"content\":user_input2}])\n",
    "    dataset = [{\"input1\": input1, \"completion1\": completion1, \"input2\": input2, \"completion2\":completion2}]\n",
    "    pd.DataFrame(dataset).to_json(battle_path, orient=\"records\", lines=True)\n",
    "    \n",
    "    # Run eval\n",
    "    !oaieval gpt-3.5-turbo battle --record_path /workspaces/evals/evallogs/logs\n",
    "    \n",
    "    # Report\n",
    "    with open(\"/workspaces/evals/evallogs/logs\", \"r\") as f:\n",
    "        events_df = pd.read_json(f, lines=True)\n",
    "\n",
    "    record_path = \"/workspaces/evals/evallogs/eval\"\n",
    "    os.makedirs(record_path, exist_ok=True)\n",
    "    events_df.to_json(os.path.join(record_path, \"events\"), lines=True, orient=\"records\")\n",
    "    events_df[\"data\"].to_json(os.path.join(record_path, \"data\"), lines=True, orient=\"records\")\n",
    "    data_df = pd.read_json(os.path.join(record_path, \"data\"), lines=True)\n",
    "    for row in data_df[0]:\n",
    "        s = str(row)\n",
    "        s = s.replace(\"[\",\"\",-1)\n",
    "        s = s.replace(\"]\",\"\",-1)\n",
    "        if s.startswith(\"{'choice':\"):\n",
    "            s = s.split(\"'\")[3]\n",
    "            choice = s\n",
    "        if s.startswith(\"{'prompt': {'role': 'user', 'content': 'You are comparing two responses to the following two instructions.\"):\n",
    "            sampled = s.split(\"\\\\n\\\\nReasoning:\\'}, \\'sampled\\': \")[1].replace(\"\\\\'\",\"'\").replace(\"\\\\n\",\"\\n\")\n",
    "    \n",
    "    data = {'Choice': choice, 'Instruction1': user_input1, 'Response1': completion1, 'Instruction2': user_input2, 'Response2': completion2, 'Sampled': sampled}\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def test_match(test_dataset):\n",
    "    \n",
    "    data = []\n",
    "    for t in range(0,len(test_dataset)):\n",
    "        data.append({\"input\":[{\"role\":\"system\",\"content\":\"\"},{\"role\":\"user\",\"content\":test_dataset[t][0]}], \"ideal\":test_dataset[t][2]})\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Paths. Assuming this notebook is in evals/evals\n",
    "\n",
    "    test_match_path = os.path.join(os.getcwd(), \"registry\", \"data\", \"match\")\n",
    "    os.makedirs(test_match_path, exist_ok=True)\n",
    "    test_match_path = os.path.join(test_match_path, \"samples.jsonl\")\n",
    "\n",
    "    df.to_json(test_match_path, orient=\"records\", lines=True)\n",
    "    \n",
    "    !oaieval gpt-3.5-turbo match --record_path /workspaces/evals/evallogs/logs\n",
    "    \n",
    "    events = \"/workspaces/evals/evallogs/logs\"\n",
    "    record_path = os.path.join(\"/workspaces/evals/evallogs/eval\")\n",
    "    with open(events, \"r\") as f:\n",
    "        events_df = pd.read_json(f, lines=True)\n",
    "    \n",
    "    os.makedirs(record_path, exist_ok=True)\n",
    "    events_df.to_json(os.path.join(record_path, \"events\"), lines=True, orient=\"records\")\n",
    "    events_df[\"data\"].to_json(os.path.join(record_path, \"data\"), lines=True, orient=\"records\")\n",
    "    data_df = pd.read_json(os.path.join(record_path, \"data\"), lines=True)\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "def content_to_list(content, dataset):\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    return dataset[dataset_df[0].to_list().index(content)]\n",
    "\n",
    "def append_response(content, response, dataset):\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    index = dataset_df[0].to_list().index(content)\n",
    "    dataset[index][2].append(response)\n",
    "    return len(dataset[index][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11822/2315514513.py:5: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  battles_df = pd.read_json(os.path.join(dataset_path, \"battles\"), lines=True)\n",
      "/tmp/ipykernel_11822/2315514513.py:5: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  battles_df = pd.read_json(os.path.join(dataset_path, \"battles\"), lines=True)\n",
      "/tmp/ipykernel_11822/2315514513.py:5: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  battles_df = pd.read_json(os.path.join(dataset_path, \"battles\"), lines=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-26 21:07:10,308] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-26 21:07:11,083] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-26 21:07:11,085] [oaieval.py:138] \u001b[1;35mRun started: 2307262107114DEF4WWZ\u001b[0m\n",
      "[2023-07-26 21:07:11,086] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-26 21:07:11,087] [eval.py:33] Evaluating 8 samples\n",
      "[2023-07-26 21:07:11,091] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:01<00:00,  4.62it/s]\n",
      "[2023-07-26 21:07:12,849] [record.py:341] Final report: {'accuracy': 0.5, 'boostrap_std': 0.19537000153554793}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-26 21:07:12,849] [oaieval.py:177] Final report:\n",
      "[2023-07-26 21:07:12,849] [oaieval.py:179] accuracy: 0.5\n",
      "[2023-07-26 21:07:12,849] [oaieval.py:179] boostrap_std: 0.19537000153554793\n",
      "[2023-07-26 21:07:12,853] [record.py:330] Logged 16 rows of events to /workspaces/evals/evallogs/logs: insert_time=2.421ms\n",
      "[2023-07-26 21:07:14,104] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-26 21:07:14,799] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-26 21:07:14,800] [oaieval.py:138] \u001b[1;35mRun started: 23072621071442OLMEID\u001b[0m\n",
      "[2023-07-26 21:07:14,802] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-26 21:07:14,829] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-26 21:07:14,829] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-26 21:07:14,830] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-26 21:07:14,835] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.28s/it]\n",
      "[2023-07-26 21:07:20,118] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-26 21:07:20,118] [oaieval.py:177] Final report:\n",
      "[2023-07-26 21:07:20,118] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-26 21:07:20,118] [oaieval.py:179] score: 0.0\n",
      "[2023-07-26 21:07:20,119] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.557ms\n",
      "[2023-07-26 21:07:21,369] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-26 21:07:22,364] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-26 21:07:22,366] [oaieval.py:138] \u001b[1;35mRun started: 230726210722RO45S5XT\u001b[0m\n",
      "[2023-07-26 21:07:22,367] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-26 21:07:22,434] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-26 21:07:22,435] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-26 21:07:22,435] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-26 21:07:22,458] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.14s/it]\n",
      "[2023-07-26 21:07:27,602] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-26 21:07:27,602] [oaieval.py:177] Final report:\n",
      "[2023-07-26 21:07:27,602] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-26 21:07:27,602] [oaieval.py:179] score: 0.0\n",
      "[2023-07-26 21:07:27,604] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.542ms\n",
      "[2023-07-26 21:07:28,795] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-26 21:07:29,603] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-26 21:07:29,604] [oaieval.py:138] \u001b[1;35mRun started: 230726210729PZV5SUUE\u001b[0m\n",
      "[2023-07-26 21:07:29,606] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-26 21:07:29,606] [eval.py:33] Evaluating 10 samples\n",
      "[2023-07-26 21:07:29,610] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  5.56it/s]\n",
      "[2023-07-26 21:07:31,437] [record.py:341] Final report: {'accuracy': 0.5, 'boostrap_std': 0.16391595407403148}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-26 21:07:31,437] [oaieval.py:177] Final report:\n",
      "[2023-07-26 21:07:31,438] [oaieval.py:179] accuracy: 0.5\n",
      "[2023-07-26 21:07:31,438] [oaieval.py:179] boostrap_std: 0.16391595407403148\n",
      "[2023-07-26 21:07:31,442] [record.py:330] Logged 20 rows of events to /workspaces/evals/evallogs/logs: insert_time=3.498ms\n",
      "[2023-07-26 21:07:32,763] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-26 21:07:33,480] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-26 21:07:33,482] [oaieval.py:138] \u001b[1;35mRun started: 230726210733XYBU2GQK\u001b[0m\n",
      "[2023-07-26 21:07:33,483] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-26 21:07:33,510] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-26 21:07:33,510] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-26 21:07:33,510] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-26 21:07:33,517] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.94s/it]\n",
      "[2023-07-26 21:07:39,462] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-26 21:07:39,463] [oaieval.py:177] Final report:\n",
      "[2023-07-26 21:07:39,463] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-26 21:07:39,463] [oaieval.py:179] score: 0.0\n",
      "[2023-07-26 21:07:39,464] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.542ms\n",
      "[2023-07-26 21:07:40,693] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-26 21:07:41,429] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-26 21:07:41,430] [oaieval.py:138] \u001b[1;35mRun started: 230726210741ZLAD2VYN\u001b[0m\n",
      "[2023-07-26 21:07:41,432] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-26 21:07:41,481] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-26 21:07:41,481] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-26 21:07:41,482] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-26 21:07:41,489] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.82s/it]\n",
      "[2023-07-26 21:07:47,315] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-26 21:07:47,315] [oaieval.py:177] Final report:\n",
      "[2023-07-26 21:07:47,315] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-26 21:07:47,315] [oaieval.py:179] score: 0.0\n",
      "[2023-07-26 21:07:47,316] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.575ms\n",
      "[2023-07-26 21:07:48,543] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-26 21:07:49,223] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-26 21:07:49,224] [oaieval.py:138] \u001b[1;35mRun started: 230726210749T3PLPNMI\u001b[0m\n",
      "[2023-07-26 21:07:49,225] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-26 21:07:49,225] [eval.py:33] Evaluating 12 samples\n",
      "[2023-07-26 21:07:49,230] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|███████████████████████████████████████████| 12/12 [00:02<00:00,  5.11it/s]\n",
      "[2023-07-26 21:07:51,609] [record.py:341] Final report: {'accuracy': 0.5833333333333334, 'boostrap_std': 0.1509230856356236}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-26 21:07:51,609] [oaieval.py:177] Final report:\n",
      "[2023-07-26 21:07:51,609] [oaieval.py:179] accuracy: 0.5833333333333334\n",
      "[2023-07-26 21:07:51,609] [oaieval.py:179] boostrap_std: 0.1509230856356236\n",
      "[2023-07-26 21:07:51,612] [record.py:330] Logged 24 rows of events to /workspaces/evals/evallogs/logs: insert_time=2.207ms\n",
      "[2023-07-26 21:07:52,805] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-26 21:07:53,672] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-26 21:07:53,674] [oaieval.py:138] \u001b[1;35mRun started: 2307262107533FW4OULH\u001b[0m\n",
      "[2023-07-26 21:07:53,675] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-26 21:07:53,703] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-26 21:07:53,703] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-26 21:07:53,704] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-26 21:07:53,708] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.61s/it]\n",
      "[2023-07-26 21:07:59,326] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-26 21:07:59,326] [oaieval.py:177] Final report:\n",
      "[2023-07-26 21:07:59,326] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-26 21:07:59,327] [oaieval.py:179] score: 0.0\n",
      "[2023-07-26 21:07:59,328] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.574ms\n",
      "[2023-07-26 21:08:00,567] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-26 21:08:01,291] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-26 21:08:01,293] [oaieval.py:138] \u001b[1;35mRun started: 230726210801FDXIZIIV\u001b[0m\n",
      "[2023-07-26 21:08:01,295] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-26 21:08:01,338] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-26 21:08:01,339] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-26 21:08:01,339] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-26 21:08:01,347] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.99s/it]\n",
      "[2023-07-26 21:08:06,337] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-26 21:08:06,337] [oaieval.py:177] Final report:\n",
      "[2023-07-26 21:08:06,337] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-26 21:08:06,338] [oaieval.py:179] score: 0.0\n",
      "[2023-07-26 21:08:06,339] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.548ms\n",
      "[2023-07-26 21:08:07,573] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-26 21:08:08,243] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-26 21:08:08,245] [oaieval.py:138] \u001b[1;35mRun started: 2307262108084EBLQ3NQ\u001b[0m\n",
      "[2023-07-26 21:08:08,246] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-26 21:08:08,276] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-26 21:08:08,276] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-26 21:08:08,278] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-26 21:08:08,285] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.98s/it]\n",
      "[2023-07-26 21:08:13,269] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-26 21:08:13,269] [oaieval.py:177] Final report:\n",
      "[2023-07-26 21:08:13,269] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-26 21:08:13,269] [oaieval.py:179] score: 0.0\n",
      "[2023-07-26 21:08:13,271] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.547ms\n",
      "[2023-07-26 21:08:14,588] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-26 21:08:15,324] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-26 21:08:15,325] [oaieval.py:138] \u001b[1;35mRun started: 230726210815VOKOZ6B4\u001b[0m\n",
      "[2023-07-26 21:08:15,326] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-26 21:08:15,327] [eval.py:33] Evaluating 15 samples\n",
      "[2023-07-26 21:08:15,331] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|███████████████████████████████████████████| 15/15 [00:03<00:00,  4.23it/s]\n",
      "[2023-07-26 21:08:18,918] [record.py:341] Final report: {'accuracy': 0.5333333333333333, 'boostrap_std': 0.14674008675622516}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-26 21:08:18,919] [oaieval.py:177] Final report:\n",
      "[2023-07-26 21:08:18,919] [oaieval.py:179] accuracy: 0.5333333333333333\n",
      "[2023-07-26 21:08:18,919] [oaieval.py:179] boostrap_std: 0.14674008675622516\n",
      "[2023-07-26 21:08:18,923] [record.py:330] Logged 30 rows of events to /workspaces/evals/evallogs/logs: insert_time=3.051ms\n",
      "[2023-07-26 21:08:20,152] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-26 21:08:20,846] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-26 21:08:20,847] [oaieval.py:138] \u001b[1;35mRun started: 230726210820ESVISK4B\u001b[0m\n",
      "[2023-07-26 21:08:20,849] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-26 21:08:20,876] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-26 21:08:20,876] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-26 21:08:20,877] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-26 21:08:20,881] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.77s/it]\n",
      "[2023-07-26 21:08:25,652] [record.py:341] Final report: {'counts/Yes': 1, 'score': 1.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-26 21:08:25,652] [oaieval.py:177] Final report:\n",
      "[2023-07-26 21:08:25,652] [oaieval.py:179] counts/Yes: 1\n",
      "[2023-07-26 21:08:25,652] [oaieval.py:179] score: 1.0\n",
      "[2023-07-26 21:08:25,654] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.674ms\n",
      "[2023-07-26 21:08:26,911] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-26 21:08:27,680] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-26 21:08:27,681] [oaieval.py:138] \u001b[1;35mRun started: 230726210827WX2TVR25\u001b[0m\n",
      "[2023-07-26 21:08:27,683] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-26 21:08:27,712] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-26 21:08:27,712] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-26 21:08:27,714] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-26 21:08:27,718] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.42s/it]\n",
      "[2023-07-26 21:08:33,139] [record.py:341] Final report: {'counts/Yes': 1, 'score': 1.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-26 21:08:33,140] [oaieval.py:177] Final report:\n",
      "[2023-07-26 21:08:33,140] [oaieval.py:179] counts/Yes: 1\n",
      "[2023-07-26 21:08:33,140] [oaieval.py:179] score: 1.0\n",
      "[2023-07-26 21:08:33,141] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.534ms\n",
      "[2023-07-26 21:08:34,453] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-26 21:08:35,158] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-26 21:08:35,160] [oaieval.py:138] \u001b[1;35mRun started: 230726210835RKXWR5BP\u001b[0m\n",
      "[2023-07-26 21:08:35,161] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-26 21:08:35,189] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-26 21:08:35,190] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-26 21:08:35,190] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-26 21:08:35,196] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.27s/it]\n",
      "[2023-07-26 21:08:42,470] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-26 21:08:42,470] [oaieval.py:177] Final report:\n",
      "[2023-07-26 21:08:42,470] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-26 21:08:42,470] [oaieval.py:179] score: 0.0\n",
      "[2023-07-26 21:08:42,472] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.509ms\n"
     ]
    }
   ],
   "source": [
    "dataset_df = pd.read_json(os.path.join(dataset_path, \"dataset\"), lines=True)\n",
    "test_dataset_df = pd.read_json(os.path.join(dataset_path, \"test_dataset\"), lines=True)\n",
    "best_response_df = pd.read_json(os.path.join(dataset_path, \"best_response\"), lines=True)\n",
    "best_response_input_df = pd.read_json(os.path.join(dataset_path, \"best_response_input\"), lines=True)\n",
    "battles_df = pd.read_json(os.path.join(dataset_path, \"battles\"), lines=True)\n",
    "n_responses_df = pd.read_json(os.path.join(dataset_path, \"n_responses\"), lines=True)\n",
    "test_match_dataset_df = pd.read_json(os.path.join(dataset_path, \"test_match_dataset\"), lines=True)\n",
    "\n",
    "dataset = dataset_df.values.tolist()\n",
    "test_dataset = test_dataset_df.values.tolist()\n",
    "best_response = best_response_df.values.tolist()\n",
    "best_response_input = best_response_input_df.values.tolist()\n",
    "n_responses = n_responses_df.values.tolist()\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "\n",
    "    test_match_data = test_match(test_dataset)\n",
    "    test_match_dataset_df = pd.concat([test_match_dataset_df, test_match_data])\n",
    "\n",
    "    same_best_response = True\n",
    "    for i in range(3,len(test_match_data[0]),2):\n",
    "        if test_match_data[0][i][\"correct\"] == False:\n",
    "            sampled = test_match_data[0][i][\"sampled\"]\n",
    "            if sampled not in dataset_df[0].to_list():\n",
    "                content = test_match_data[0][i-1][\"prompt\"][1][\"content\"]\n",
    "                dataset.append([sampled,content,[]])\n",
    "                dataset_df = pd.DataFrame(dataset)\n",
    "                n = append_response(content, sampled, dataset)\n",
    "                n_responses.append([n])\n",
    "\n",
    "                data = battle(system_input,content,sampled,system_input,best_response_input[-1][0],best_response[-1][0]) \n",
    "                battles_df = pd.concat([battles_df,pd.DataFrame(data, index=[0])], ignore_index=True)\n",
    "\n",
    "                if data[\"Choice\"] == \"Yes\":\n",
    "                    best_response_input.append([content])\n",
    "                    best_response.append([sampled])\n",
    "                    test_dataset = [content_to_list(content,dataset)]\n",
    "                    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "                    same_best_response = False\n",
    "\n",
    "    if same_best_response:\n",
    "        for t in test_dataset:\n",
    "            if t[0] != \"\":\n",
    "                if t[1] not in test_dataset_df[0].to_list():\n",
    "                    test_dataset.append(content_to_list(t[1],dataset))\n",
    "                    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "            l = content_to_list(t[0], dataset)\n",
    "            for response in l[2]:\n",
    "                if response not in test_dataset_df[0].to_list():\n",
    "                    test_dataset.append(content_to_list(response,dataset))\n",
    "                    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "    best_response_df = pd.DataFrame(best_response)\n",
    "    best_response_input_df = pd.DataFrame(best_response_input)\n",
    "    n_responses_df = pd.DataFrame(n_responses)  \n",
    "    \n",
    "    dataset_df.to_json(os.path.join(dataset_path, \"dataset\"), lines=True, orient=\"records\")\n",
    "    test_dataset_df.to_json(os.path.join(dataset_path, \"test_dataset\"), lines=True, orient=\"records\")\n",
    "    best_response_df.to_json(os.path.join(dataset_path, \"best_response\"), lines=True, orient=\"records\")\n",
    "    best_response_input_df.to_json(os.path.join(dataset_path, \"best_response_input\"), lines=True, orient=\"records\")\n",
    "    n_responses_df.to_json(os.path.join(dataset_path, \"n_responses\"), lines=True, orient=\"records\")\n",
    "\n",
    "    dataset_df[0].to_json(os.path.join(dataset_path, \"dataset0\"), lines=True, orient=\"records\")\n",
    "    dataset_df[1].to_json(os.path.join(dataset_path, \"dataset1\"), lines=True, orient=\"records\")\n",
    "\n",
    "    battles_df.to_json(os.path.join(dataset_path, \"battles\"), lines=True, orient=\"records\")\n",
    "    test_match_dataset_df.to_json(os.path.join(dataset_path, \"test_match_dataset\"), lines=True, orient=\"records\")\n",
    "\n",
    "    responses = []\n",
    "    for responses_list in dataset_df[2]:\n",
    "        responses.append(len(responses_list))\n",
    "        pd.Series(responses)\n",
    "    responses_df = pd.DataFrame(responses, dataset_df[2])\n",
    "    responses_df.to_json(os.path.join(dataset_path, \"responses\"), lines=True, orient=\"records\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     False\n",
       "1     False\n",
       "2     False\n",
       "3     False\n",
       "4     False\n",
       "5     False\n",
       "6     False\n",
       "7     False\n",
       "8     False\n",
       "9     False\n",
       "10    False\n",
       "11    False\n",
       "12    False\n",
       "13    False\n",
       "14    False\n",
       "15    False\n",
       "16    False\n",
       "17    False\n",
       "Name: 0, dtype: bool"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df[0].duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Choice</th>\n",
       "      <th>Instruction1</th>\n",
       "      <th>Response1</th>\n",
       "      <th>Instruction2</th>\n",
       "      <th>Response2</th>\n",
       "      <th>Sampled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>I'm sorry, I cannot continue the text for you ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\"1. The first response acknowledges the user's...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Choice Instruction1                                          Response1  \\\n",
       "0    Yes               I'm sorry, I cannot continue the text for you ...   \n",
       "\n",
       "  Instruction2 Response2                                            Sampled  \n",
       "0                         \"1. The first response acknowledges the user's...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {1,2,3,4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_to_list(best_response[-1][0],dataset)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Yes',\n",
       "  '',\n",
       "  \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  '',\n",
       "  '',\n",
       "  '\"1. The first response acknowledges the user\\'s request and provides a clear explanation of why it cannot fulfill the request.\\n2. The second response is incomplete and does not provide any information or explanation.\\n\\nBased on this reasoning, the first response is better than the second.\\n\\nYes\"}'],\n",
       " ['Yes',\n",
       "  \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  \"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\",\n",
       "  '',\n",
       "  \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  '\"1. Both responses acknowledge the user\\'s statement and provide a similar explanation for why the AI cannot continue the text.\\n2. Response 1 offers additional assistance and invites the user to ask any other questions or seek help with something else.\\n3. Response 2 simply restates the same explanation without any additional information or offer of assistance.\\n\\nBased on this reasoning, the first response is better than the second.\\n\\nYes\"}'],\n",
       " ['No',\n",
       "  \"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\",\n",
       "  'Thank you! I appreciate your willingness to help. I will definitely reach out if I have any further questions or need assistance.',\n",
       "  \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  \"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\",\n",
       "  \"'1. The first response acknowledges and appreciates the willingness to help, which shows gratitude and politeness.\\n2. The first response indicates that the user will reach out if they have any further questions or need assistance, which shows a clear understanding of how to seek help.\\n3. The second response is a repetition of the initial instruction, which may not provide any additional value to the user.\\n\\nBased on these points, the first response is better than the second.\\n\\nNo'}\"],\n",
       " ['No',\n",
       "  \"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\",\n",
       "  'Thank you! I appreciate your willingness to assist. I will definitely reach out if I have any further questions or need help with anything else.',\n",
       "  \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  \"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\",\n",
       "  \"'1. The first response acknowledges and appreciates the willingness to assist, showing gratitude.\\n2. The first response indicates that the user will reach out if they have any further questions or need help with anything else.\\n3. The second response simply repeats the initial instruction without any additional information or acknowledgement.\\n\\nBased on this reasoning, the first response is better than the second.\\n\\nNo'}\"],\n",
       " ['No',\n",
       "  'Thank you! I appreciate your willingness to help. I will definitely reach out if I have any further questions or need assistance.',\n",
       "  \"You're welcome! I'm glad I could help. Feel free to reach out anytime you need assistance or have more questions. I'm here to help. Have a great day!\",\n",
       "  \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  \"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\",\n",
       "  '\"1. Both responses acknowledge the user\\'s gratitude and express willingness to help.\\n2. Both responses encourage the user to reach out if they have any further questions or need assistance.\\n3. Both responses convey that the AI language model is available to help.\\n4. Both responses end with a positive note, wishing the user a great day or assuring them that the AI is there to help.\\n\\nBased on the above reasoning, it can be concluded that both responses are equally good. \\n\\nNo\"}'],\n",
       " ['No',\n",
       "  \"You're welcome! I'm glad I could help. Feel free to reach out anytime you need assistance or have more questions. I'm here to help. Have a great day!\",\n",
       "  \"Thank you! I appreciate your offer to help. I'll definitely keep that in mind and reach out if I need any further assistance. Have a wonderful day too!\",\n",
       "  \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  \"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\",\n",
       "  '\"1. Both responses acknowledge the user\\'s message and express gratitude.\\n2. Both responses offer assistance and encourage the user to reach out if they have any further questions or need help.\\n3. Both responses end with a positive and friendly closing statement.\\n\\nBased on the above reasoning, it can be concluded that both responses are equally good. \\n\\nNo\"}'],\n",
       " ['No',\n",
       "  'Thank you! I appreciate your willingness to assist. I will definitely reach out if I have any further questions or need help with anything else.',\n",
       "  \"You're welcome! I'm glad I could help. Feel free to reach out anytime you need assistance or have more questions. I'm here to help. Have a great day!\",\n",
       "  \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  \"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\",\n",
       "  '\"1. Both responses acknowledge the user\\'s gratitude and appreciation.\\n2. Both responses offer assistance and encourage the user to reach out if they have any further questions or need help.\\n3. Both responses express availability and willingness to help.\\n4. Both responses end on a positive note, wishing the user a great day or offering further assistance.\\n\\nBased on the above reasoning, it can be concluded that both responses are equally good. \\n\\nNo\"}'],\n",
       " ['No',\n",
       "  \"Thank you! I appreciate your offer to help. I'll definitely keep that in mind and reach out if I need any further assistance. Have a wonderful day too!\",\n",
       "  \"You're welcome! I'm glad I could offer my help. Don't hesitate to reach out if you need anything in the future. Have a fantastic day as well!\",\n",
       "  \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  \"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\",\n",
       "  '\"1. Both responses acknowledge the user\\'s message and express gratitude or understanding.\\n2. Response 1 offers further assistance and encourages the user to reach out if needed, while Response 2 offers assistance with other questions or needs.\\n3. Response 1 ends with a positive and friendly wish for the user to have a fantastic day, while Response 2 does not include a similar closing statement.\\n\\nBased on these points, the first response is better than the second because it provides more specific and helpful information, and ends with a positive and friendly note.\\n\\nNo\"}']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "battles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['',\n",
       "  None,\n",
       "  [\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\"]],\n",
       " [\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  '',\n",
       "  [\"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\"]],\n",
       " [\"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\",\n",
       "  \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  ['Thank you! I appreciate your willingness to help. I will definitely reach out if I have any further questions or need assistance.',\n",
       "   'Thank you! I appreciate your willingness to assist. I will definitely reach out if I have any further questions or need help with anything else.']],\n",
       " ['Thank you! I appreciate your willingness to help. I will definitely reach out if I have any further questions or need assistance.',\n",
       "  \"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\",\n",
       "  [\"You're welcome! I'm glad I could help. Feel free to reach out anytime you need assistance or have more questions. I'm here to help. Have a great day!\"]],\n",
       " ['Thank you! I appreciate your willingness to assist. I will definitely reach out if I have any further questions or need help with anything else.',\n",
       "  \"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\",\n",
       "  [\"You're welcome! I'm glad I could help. Feel free to reach out anytime you need assistance or have more questions. I'm here to help. Have a great day!\"]],\n",
       " [\"You're welcome! I'm glad I could help. Feel free to reach out anytime you need assistance or have more questions. I'm here to help. Have a great day!\",\n",
       "  'Thank you! I appreciate your willingness to help. I will definitely reach out if I have any further questions or need assistance.',\n",
       "  [\"Thank you! I appreciate your offer to help. I'll definitely keep that in mind and reach out if I need any further assistance. Have a wonderful day too!\"]],\n",
       " [\"Thank you! I appreciate your offer to help. I'll definitely keep that in mind and reach out if I need any further assistance. Have a wonderful day too!\",\n",
       "  \"You're welcome! I'm glad I could help. Feel free to reach out anytime you need assistance or have more questions. I'm here to help. Have a great day!\",\n",
       "  [\"You're welcome! I'm glad I could offer my help. Don't hesitate to reach out if you need anything in the future. Have a fantastic day as well!\"]],\n",
       " [\"You're welcome! I'm glad I could help. Feel free to reach out anytime you need assistance or have more questions. I'm here to help. Have a great day!\",\n",
       "  'Thank you! I appreciate your willingness to assist. I will definitely reach out if I have any further questions or need help with anything else.',\n",
       "  []],\n",
       " [\"You're welcome! I'm glad I could offer my help. Don't hesitate to reach out if you need anything in the future. Have a fantastic day as well!\",\n",
       "  \"Thank you! I appreciate your offer to help. I'll definitely keep that in mind and reach out if I need any further assistance. Have a wonderful day too!\",\n",
       "  []]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(test_match_data[0])-2)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " None,\n",
       " [\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate content on behalf of individuals.\",\n",
       "  \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\"]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df[0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       " \"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  '',\n",
       "  [\"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\"]],\n",
       " ['',\n",
       "  None,\n",
       "  [\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\"]],\n",
       " [\"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\",\n",
       "  \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  []]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'prompt': [{'role': 'system', 'content': ''},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'correct': False, 'expected': [], 'picked': N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                                               None\n",
       "1                                               None\n",
       "2  {'prompt': [{'role': 'system', 'content': ''},...\n",
       "3  {'correct': False, 'expected': [], 'picked': N..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_match_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=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\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "test_match() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(dataset_test)):\n\u001b[1;32m      3\u001b[0m     test_match_data \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m     test_match_data\u001b[39m.\u001b[39mappend(test_match(dataset_test[d][\u001b[39m0\u001b[39;49m],dataset_test[d][\u001b[39m2\u001b[39;49m]))\n\u001b[1;32m      5\u001b[0m dataset_test\u001b[39m.\u001b[39mappend(dataset(dataset[:][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mindex(dataset_test[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m])))\n",
      "\u001b[0;31mTypeError\u001b[0m: test_match() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "for d in range(0,len(dataset_test)):\n",
    "    test_match_data = []\n",
    "    test_match_data.append(test_match(dataset_test[d][0],dataset_test[d][2]))\n",
    "dataset_test.append(dataset(dataset[:][0].index(dataset_test[0][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.append([])\n",
    "for u in user_content:\n",
    "    data = battle(system_input1,u,system_input2,best_response_input)\n",
    "    if data[\"Choice\"] == \"Yes\":\n",
    "        best_response_input = data[\"Instruction1\"]\n",
    "        best_response.append(data[\"Response1\"]) \n",
    "    dataset[u].append(data)\n",
    "user_content.append(data[\"Response1\"])\n",
    "df = pd.DataFrame(dataset)\n",
    "user_content_df = pd.DataFrame(user_content)\n",
    "df.to_json(os.path.join(os.getcwd(), \"battles.jsonl\"), orient=\"records\", lines=True)\n",
    "user_content_df.to_json(os.path.join(os.getcwd(), \"battles_user_content.jsonl\"), orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-shot prompt\n",
    "\n",
    "choice = int(df[\"Choice\"][0])-1\n",
    "if choice == 1:\n",
    "    input[0] = [\n",
    "        {\"role\": \"system\", \"content\": user_input[choice], \"name\": \"example_user\"},\n",
    "        {\"role\": \"system\", \"content\": df[df.columns[1+(2*choice)]][0], \"name\": \"example_assistant\"},\n",
    "        {\"role\":\"system\",\"content\":\"\"},\n",
    "        {\"role\":\"user\",\"content\":user_input[1]},\n",
    "        ]\n",
    "    dataset = [{\"input1\": input[0], \"input2\": input[1]}]\n",
    "    df = pd.DataFrame(dataset)\n",
    "    df.to_json(os.path.join(os.getcwd(), \"battles.jsonl\"), orient=\"records\", lines=True)\n",
    "    df.to_json(os.path.join(os.getcwd(), \"battles.jsonl\"), orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instruction(i):\n",
    "    if i.startswith(\"User: \"):\n",
    "        input = [{\"role\":\"system\",\"content\":\"\"},{\"role\":\"user\",\"content\":i.removeprefix(\"User: \")}]\n",
    "    else:\n",
    "        ins = i.split(\"\\nUser: \")\n",
    "        input = [{\"role\":\"system\",\"content\":ins[0]},{\"role\":\"user\",\"content\":ins[1]}]\n",
    "    return input\n",
    "meta = []\n",
    "for index, row in df.iterrows():\n",
    "    meta.append({\"input1\": instruction(row[0]), \"completion1\": row[1], \"input2\": instruction(row[2]), \"completion2\": row[3], \"choice\":\"\"})\n",
    "meta = pd.DataFrame(meta)\n",
    "\n",
    "row = 5\n",
    "meta.iloc[row][0],meta.iloc[row][1],meta.iloc[row][2],meta.iloc[row][3]\n",
    "\n",
    "meta.iloc[row][4] = \"1\"\n",
    "#meta.to_json(meta_path, orient=\"records\", lines=True)\n",
    "!oaieval gpt-3.5-turbo dataset-meta --record_path /workspaces/evals/evallogs/logs-meta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
