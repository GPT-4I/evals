{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1473/3990185712.py:90: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  battles_df = pd.read_json(os.path.join(dataset_path, \"battles\"), lines=True)\n",
      "/tmp/ipykernel_1473/3990185712.py:90: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  battles_df = pd.read_json(os.path.join(dataset_path, \"battles\"), lines=True)\n",
      "/tmp/ipykernel_1473/3990185712.py:90: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  battles_df = pd.read_json(os.path.join(dataset_path, \"battles\"), lines=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-27 12:27:04,363] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:27:05,126] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:27:05,128] [oaieval.py:138] \u001b[1;35mRun started: 230727122705VAVB4FOF\u001b[0m\n",
      "[2023-07-27 12:27:05,129] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-27 12:27:05,129] [eval.py:33] Evaluating 21 samples\n",
      "[2023-07-27 12:27:05,134] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|███████████████████████████████████████████| 21/21 [00:08<00:00,  2.46it/s]\n",
      "[2023-07-27 12:27:13,713] [record.py:341] Final report: {'accuracy': 0.6666666666666666, 'boostrap_std': 0.11328508286619206}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:27:13,713] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:27:13,713] [oaieval.py:179] accuracy: 0.6666666666666666\n",
      "[2023-07-27 12:27:13,713] [oaieval.py:179] boostrap_std: 0.11328508286619206\n",
      "[2023-07-27 12:27:13,718] [record.py:330] Logged 42 rows of events to /workspaces/evals/evallogs/logs: insert_time=4.469ms\n",
      "[2023-07-27 12:27:14,905] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:27:15,659] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:27:15,660] [oaieval.py:138] \u001b[1;35mRun started: 230727122715BWPTI3PJ\u001b[0m\n",
      "[2023-07-27 12:27:15,662] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:27:15,698] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:27:15,698] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:27:15,698] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:27:15,704] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.47s/it]\n",
      "[2023-07-27 12:27:23,181] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:27:23,181] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:27:23,181] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:27:23,181] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:27:23,183] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.642ms\n",
      "[2023-07-27 12:27:24,491] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:27:25,170] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:27:25,171] [oaieval.py:138] \u001b[1;35mRun started: 230727122725CX3DRVTS\u001b[0m\n",
      "[2023-07-27 12:27:25,173] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:27:25,199] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:27:25,200] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:27:25,200] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:27:25,205] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.17s/it]\n",
      "[2023-07-27 12:27:33,378] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:27:33,378] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:27:33,378] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:27:33,378] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:27:33,380] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.658ms\n",
      "[2023-07-27 12:27:34,660] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:27:35,340] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:27:35,341] [oaieval.py:138] \u001b[1;35mRun started: 230727122735MEA24WUS\u001b[0m\n",
      "[2023-07-27 12:27:35,343] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:27:35,373] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:27:35,374] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:27:35,374] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:27:35,379] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:09<00:00,  9.33s/it]\n",
      "[2023-07-27 12:27:44,709] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:27:44,709] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:27:44,709] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:27:44,709] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:27:44,711] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.500ms\n",
      "[2023-07-27 12:27:45,929] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:27:46,646] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:27:46,647] [oaieval.py:138] \u001b[1;35mRun started: 230727122746MXXBJYEM\u001b[0m\n",
      "[2023-07-27 12:27:46,649] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:27:46,679] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:27:46,680] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:27:46,680] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:27:46,685] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.62s/it]\n",
      "[2023-07-27 12:27:54,308] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:27:54,308] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:27:54,308] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:27:54,308] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:27:54,310] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.473ms\n",
      "[2023-07-27 12:27:55,606] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:27:56,373] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:27:56,374] [oaieval.py:138] \u001b[1;35mRun started: 230727122756SAIYRNSQ\u001b[0m\n",
      "[2023-07-27 12:27:56,375] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:27:56,405] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:27:56,405] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:27:56,406] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:27:56,410] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.07s/it]\n",
      "[2023-07-27 12:28:03,485] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:28:03,486] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:28:03,486] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:28:03,486] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:28:03,487] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.530ms\n",
      "[2023-07-27 12:28:04,699] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:28:05,452] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:28:05,454] [oaieval.py:138] \u001b[1;35mRun started: 2307271228055BAY2NH4\u001b[0m\n",
      "[2023-07-27 12:28:05,455] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:28:05,487] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:28:05,488] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:28:05,488] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:28:05,492] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.30s/it]\n",
      "[2023-07-27 12:28:13,792] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:28:13,793] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:28:13,793] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:28:13,793] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:28:13,795] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.934ms\n",
      "[2023-07-27 12:28:15,024] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:28:15,736] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:28:15,738] [oaieval.py:138] \u001b[1;35mRun started: 230727122815ODMHEM3W\u001b[0m\n",
      "[2023-07-27 12:28:15,739] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-27 12:28:15,739] [eval.py:33] Evaluating 30 samples\n",
      "[2023-07-27 12:28:15,746] [eval.py:139] Running in threaded mode with 10 threads!\n",
      " 97%|█████████████████████████████████████████▌ | 29/30 [00:13<00:00,  1.66it/s][2023-07-27 12:28:46,136] [_common.py:105] Backing off openai_chat_completion_create_retrying(...) for 0.4s (openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.)\n",
      "100%|███████████████████████████████████████████| 30/30 [00:34<00:00,  1.13s/it]\n",
      "[2023-07-27 12:28:49,805] [record.py:341] Final report: {'accuracy': 0.6, 'boostrap_std': 0.08719773187672052}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:28:49,805] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:28:49,805] [oaieval.py:179] accuracy: 0.6\n",
      "[2023-07-27 12:28:49,805] [oaieval.py:179] boostrap_std: 0.08719773187672052\n",
      "[2023-07-27 12:28:49,814] [record.py:330] Logged 60 rows of events to /workspaces/evals/evallogs/logs: insert_time=6.502ms\n",
      "[2023-07-27 12:28:51,034] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:28:51,780] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:28:51,781] [oaieval.py:138] \u001b[1;35mRun started: 230727122851Q2KBRU6B\u001b[0m\n",
      "[2023-07-27 12:28:51,783] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:28:51,813] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:28:51,813] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:28:51,814] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:28:51,819] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.59s/it]\n",
      "[2023-07-27 12:28:59,415] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:28:59,415] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:28:59,415] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:28:59,415] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:28:59,417] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.471ms\n",
      "[2023-07-27 12:29:00,606] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:29:01,328] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:29:01,329] [oaieval.py:138] \u001b[1;35mRun started: 2307271229017ISUP7AT\u001b[0m\n",
      "[2023-07-27 12:29:01,331] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:29:01,359] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:29:01,359] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:29:01,360] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:29:01,365] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.55s/it]\n",
      "[2023-07-27 12:29:08,919] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:29:08,919] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:29:08,919] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:29:08,919] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:29:08,921] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.562ms\n",
      "[2023-07-27 12:29:10,168] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:29:10,912] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:29:10,914] [oaieval.py:138] \u001b[1;35mRun started: 230727122910JBNA263A\u001b[0m\n",
      "[2023-07-27 12:29:10,915] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:29:10,944] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:29:10,944] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:29:10,944] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:29:10,950] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.81s/it]\n",
      "[2023-07-27 12:29:17,767] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:29:17,767] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:29:17,767] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:29:17,767] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:29:17,769] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.559ms\n",
      "[2023-07-27 12:29:19,050] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:29:19,784] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:29:19,785] [oaieval.py:138] \u001b[1;35mRun started: 230727122919MYIRYAYP\u001b[0m\n",
      "[2023-07-27 12:29:19,786] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:29:19,816] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:29:19,816] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:29:19,817] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:29:19,822] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.45s/it]\n",
      "[2023-07-27 12:29:28,278] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:29:28,278] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:29:28,278] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:29:28,278] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:29:28,280] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.658ms\n",
      "[2023-07-27 12:29:29,522] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:29:30,263] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:29:30,264] [oaieval.py:138] \u001b[1;35mRun started: 230727122930BT5VW2WA\u001b[0m\n",
      "[2023-07-27 12:29:30,266] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:29:30,295] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:29:30,295] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:29:30,296] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:29:30,300] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.90s/it]\n",
      "[2023-07-27 12:29:41,208] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:29:41,208] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:29:41,208] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:29:41,208] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:29:41,210] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.492ms\n",
      "[2023-07-27 12:29:42,409] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:29:43,183] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:29:43,185] [oaieval.py:138] \u001b[1;35mRun started: 230727122943T3JYGECL\u001b[0m\n",
      "[2023-07-27 12:29:43,186] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:29:43,214] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:29:43,214] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:29:43,215] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:29:43,220] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.65s/it]\n",
      "[2023-07-27 12:29:51,881] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:29:51,881] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:29:51,882] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:29:51,882] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:29:51,883] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.599ms\n",
      "[2023-07-27 12:29:53,103] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:29:53,788] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:29:53,789] [oaieval.py:138] \u001b[1;35mRun started: 230727122953HYH2Q64M\u001b[0m\n",
      "[2023-07-27 12:29:53,791] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:29:53,819] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:29:53,819] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:29:53,820] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:29:53,825] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.37s/it]\n",
      "[2023-07-27 12:30:01,203] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:30:01,203] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:30:01,203] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:30:01,203] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:30:01,205] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.619ms\n",
      "[2023-07-27 12:30:02,433] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:30:03,111] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:30:03,112] [oaieval.py:138] \u001b[1;35mRun started: 230727123003GO264MSH\u001b[0m\n",
      "[2023-07-27 12:30:03,114] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:30:03,142] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:30:03,142] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:30:03,142] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:30:03,147] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.77s/it]\n",
      "[2023-07-27 12:30:10,919] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:30:10,919] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:30:10,919] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:30:10,919] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:30:10,922] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=1.332ms\n",
      "[2023-07-27 12:30:12,218] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:30:12,954] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:30:12,956] [oaieval.py:138] \u001b[1;35mRun started: 230727123012XMUJLORC\u001b[0m\n",
      "[2023-07-27 12:30:12,957] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-27 12:30:12,958] [eval.py:33] Evaluating 41 samples\n",
      "[2023-07-27 12:30:12,963] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|███████████████████████████████████████████| 41/41 [00:15<00:00,  2.66it/s]\n",
      "[2023-07-27 12:30:28,404] [record.py:341] Final report: {'accuracy': 0.6585365853658537, 'boostrap_std': 0.07185227901743965}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:30:28,404] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:30:28,404] [oaieval.py:179] accuracy: 0.6585365853658537\n",
      "[2023-07-27 12:30:28,404] [oaieval.py:179] boostrap_std: 0.07185227901743965\n",
      "[2023-07-27 12:30:28,416] [record.py:330] Logged 82 rows of events to /workspaces/evals/evallogs/logs: insert_time=11.387ms\n",
      "[2023-07-27 12:30:29,683] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:30:30,479] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:30:30,480] [oaieval.py:138] \u001b[1;35mRun started: 230727123030EKQAFPNX\u001b[0m\n",
      "[2023-07-27 12:30:30,482] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:30:30,515] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:30:30,516] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:30:30,516] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:30:30,521] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.01s/it]\n",
      "[2023-07-27 12:30:40,534] [record.py:341] Final report: {'counts/Yes': 1, 'score': 1.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:30:40,534] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:30:40,534] [oaieval.py:179] counts/Yes: 1\n",
      "[2023-07-27 12:30:40,534] [oaieval.py:179] score: 1.0\n",
      "[2023-07-27 12:30:40,536] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.692ms\n",
      "[2023-07-27 12:30:41,745] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:30:42,553] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:30:42,554] [oaieval.py:138] \u001b[1;35mRun started: 230727123042HRW6K2YB\u001b[0m\n",
      "[2023-07-27 12:30:42,556] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:30:42,583] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:30:42,583] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:30:42,584] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:30:42,590] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.21s/it]\n",
      "[2023-07-27 12:30:49,800] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:30:49,800] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:30:49,800] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:30:49,800] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:30:49,802] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.555ms\n",
      "[2023-07-27 12:30:50,986] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:30:51,703] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:30:51,705] [oaieval.py:138] \u001b[1;35mRun started: 230727123051QUXBKNOM\u001b[0m\n",
      "[2023-07-27 12:30:51,707] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:30:51,735] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:30:51,735] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:30:51,736] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:30:51,741] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.70s/it]\n",
      "[2023-07-27 12:30:58,442] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:30:58,442] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:30:58,442] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:30:58,442] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:30:58,444] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.642ms\n",
      "[2023-07-27 12:30:59,637] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:31:00,342] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:31:00,343] [oaieval.py:138] \u001b[1;35mRun started: 230727123100IOLEUCDL\u001b[0m\n",
      "[2023-07-27 12:31:00,344] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:31:00,373] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:31:00,373] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:31:00,375] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:31:00,380] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:12<00:00, 12.35s/it]\n",
      "[2023-07-27 12:31:12,738] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:31:12,739] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:31:12,739] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:31:12,739] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:31:12,740] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.499ms\n",
      "[2023-07-27 12:31:13,992] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:31:15,136] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:31:15,137] [oaieval.py:138] \u001b[1;35mRun started: 230727123115LHUXKYHS\u001b[0m\n",
      "[2023-07-27 12:31:15,139] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:31:15,172] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:31:15,172] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:31:15,172] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:31:15,177] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.43s/it]\n",
      "[2023-07-27 12:31:22,607] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:31:22,607] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:31:22,607] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:31:22,607] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:31:22,608] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.606ms\n",
      "[2023-07-27 12:31:23,912] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:31:24,685] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:31:24,686] [oaieval.py:138] \u001b[1;35mRun started: 230727123124NCBCNKHU\u001b[0m\n",
      "[2023-07-27 12:31:24,687] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:31:24,717] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:31:24,717] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:31:24,718] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:31:24,722] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.20s/it]\n",
      "[2023-07-27 12:31:32,923] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:31:32,923] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:31:32,924] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:31:32,924] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:31:32,925] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.570ms\n",
      "[2023-07-27 12:31:34,146] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:31:34,935] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:31:34,937] [oaieval.py:138] \u001b[1;35mRun started: 2307271231347NQ26MFC\u001b[0m\n",
      "[2023-07-27 12:31:34,939] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:31:34,986] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:31:34,987] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:31:34,988] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:31:34,994] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.00s/it]\n",
      "[2023-07-27 12:31:42,000] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:31:42,000] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:31:42,000] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:31:42,001] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:31:42,002] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.882ms\n",
      "[2023-07-27 12:31:43,196] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:31:44,004] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:31:44,005] [oaieval.py:138] \u001b[1;35mRun started: 230727123144ITEK2OWT\u001b[0m\n",
      "[2023-07-27 12:31:44,006] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:31:44,037] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:31:44,037] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:31:44,037] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:31:44,042] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.74s/it]\n",
      "[2023-07-27 12:31:51,783] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:31:51,783] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:31:51,783] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:31:51,783] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:31:51,784] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.610ms\n",
      "[2023-07-27 12:31:52,966] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:31:53,699] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:31:53,701] [oaieval.py:138] \u001b[1;35mRun started: 2307271231537W7KX5S4\u001b[0m\n",
      "[2023-07-27 12:31:53,702] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-27 12:31:53,748] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-27 12:31:53,748] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-27 12:31:53,749] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:31:53,756] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.08s/it]\n",
      "[2023-07-27 12:32:00,842] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:32:00,842] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:32:00,842] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-27 12:32:00,842] [oaieval.py:179] score: 0.0\n",
      "[2023-07-27 12:32:00,843] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.502ms\n",
      "[2023-07-27 12:32:02,098] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-27 12:32:02,793] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-27 12:32:02,795] [oaieval.py:138] \u001b[1;35mRun started: 230727123202LWD3N55K\u001b[0m\n",
      "[2023-07-27 12:32:02,796] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-27 12:32:02,796] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-27 12:32:02,801] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.19s/it]\n",
      "/home/codespace/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/codespace/.local/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "[2023-07-27 12:32:06,011] [record.py:341] Final report: {'accuracy': 1.0, 'boostrap_std': nan}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-27 12:32:06,012] [oaieval.py:177] Final report:\n",
      "[2023-07-27 12:32:06,012] [oaieval.py:179] accuracy: 1.0\n",
      "[2023-07-27 12:32:06,012] [oaieval.py:179] boostrap_std: nan\n",
      "[2023-07-27 12:32:06,013] [record.py:330] Logged 2 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.430ms\n"
     ]
    }
   ],
   "source": [
    "runs = 4\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"/workspaces/evals/dataset\"\n",
    "system_input = \"\"\n",
    "\n",
    "def battle(system_input1, user_input1, completion1, system_input2, user_input2, completion2):\n",
    "\n",
    "    battle_path = os.path.join(os.getcwd(), \"registry\", \"data\", \"battle\")\n",
    "    os.makedirs(battle_path, exist_ok=True)\n",
    "    battle_path = os.path.join(battle_path, \"samples.jsonl\")\n",
    "\n",
    "    # Data\n",
    "    input1 = ([{\"role\":\"system\",\"content\":system_input1},{\"role\":\"user\",\"content\":user_input1}])\n",
    "    input2 = ([{\"role\":\"system\",\"content\":system_input2},{\"role\":\"user\",\"content\":user_input2}])\n",
    "    dataset = [{\"input1\": input1, \"completion1\": completion1, \"input2\": input2, \"completion2\":completion2}]\n",
    "    pd.DataFrame(dataset).to_json(battle_path, orient=\"records\", lines=True)\n",
    "    \n",
    "    # Run eval\n",
    "    !oaieval gpt-3.5-turbo battle --record_path /workspaces/evals/evallogs/logs\n",
    "    \n",
    "    # Report\n",
    "    with open(\"/workspaces/evals/evallogs/logs\", \"r\") as f:\n",
    "        events_df = pd.read_json(f, lines=True)\n",
    "\n",
    "    record_path = \"/workspaces/evals/evallogs/eval\"\n",
    "    os.makedirs(record_path, exist_ok=True)\n",
    "    events_df.to_json(os.path.join(record_path, \"events\"), lines=True, orient=\"records\")\n",
    "    events_df[\"data\"].to_json(os.path.join(record_path, \"data\"), lines=True, orient=\"records\")\n",
    "    data_df = pd.read_json(os.path.join(record_path, \"data\"), lines=True)\n",
    "    for row in data_df[0]:\n",
    "        s = str(row)\n",
    "        s = s.replace(\"[\",\"\",-1)\n",
    "        s = s.replace(\"]\",\"\",-1)\n",
    "        if s.startswith(\"{'choice':\"):\n",
    "            s = s.split(\"'\")[3]\n",
    "            choice = s\n",
    "        if s.startswith(\"{'prompt': {'role': 'user', 'content': 'You are comparing two responses to the following two instructions.\"):\n",
    "            sampled = s.split(\"\\\\n\\\\nReasoning:\\'}, \\'sampled\\': \")[1].replace(\"\\\\'\",\"'\").replace(\"\\\\n\",\"\\n\")\n",
    "    \n",
    "    data = {'Choice': choice, 'Instruction1': user_input1, 'Response1': completion1, 'Instruction2': user_input2, 'Response2': completion2, 'Sampled': sampled}\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def test_match(test_dataset):\n",
    "    \n",
    "    data = []\n",
    "    for t in range(0,len(test_dataset)):\n",
    "        data.append({\"input\":[{\"role\":\"system\",\"content\":\"\"},{\"role\":\"user\",\"content\":test_dataset[t][0]}], \"ideal\":test_dataset[t][2]})\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Paths. Assuming this notebook is in evals/evals\n",
    "\n",
    "    test_match_path = os.path.join(os.getcwd(), \"registry\", \"data\", \"match\")\n",
    "    os.makedirs(test_match_path, exist_ok=True)\n",
    "    test_match_path = os.path.join(test_match_path, \"samples.jsonl\")\n",
    "\n",
    "    df.to_json(test_match_path, orient=\"records\", lines=True)\n",
    "    \n",
    "    !oaieval gpt-3.5-turbo match --record_path /workspaces/evals/evallogs/logs\n",
    "    \n",
    "    events = \"/workspaces/evals/evallogs/logs\"\n",
    "    record_path = os.path.join(\"/workspaces/evals/evallogs/eval\")\n",
    "    with open(events, \"r\") as f:\n",
    "        events_df = pd.read_json(f, lines=True)\n",
    "    \n",
    "    os.makedirs(record_path, exist_ok=True)\n",
    "    events_df.to_json(os.path.join(record_path, \"events\"), lines=True, orient=\"records\")\n",
    "    events_df[\"data\"].to_json(os.path.join(record_path, \"data\"), lines=True, orient=\"records\")\n",
    "    data_df = pd.read_json(os.path.join(record_path, \"data\"), lines=True)\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "def content_to_list(content, dataset):\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    return dataset[dataset_df[0].to_list().index(content)]\n",
    "\n",
    "def append_response(content, response, dataset):\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    index = dataset_df[0].to_list().index(content)\n",
    "    dataset[index][2].append(response)\n",
    "    return len(dataset[index][2])\n",
    "dataset_df = pd.read_json(os.path.join(dataset_path, \"dataset\"), lines=True)\n",
    "test_dataset_df = pd.read_json(os.path.join(dataset_path, \"test_dataset\"), lines=True)\n",
    "best_response_df = pd.read_json(os.path.join(dataset_path, \"best_response\"), lines=True)\n",
    "best_response_input_df = pd.read_json(os.path.join(dataset_path, \"best_response_input\"), lines=True)\n",
    "battles_df = pd.read_json(os.path.join(dataset_path, \"battles\"), lines=True)\n",
    "n_responses_df = pd.read_json(os.path.join(dataset_path, \"n_responses\"), lines=True)\n",
    "test_match_dataset_df = pd.read_json(os.path.join(dataset_path, \"test_match_dataset\"), lines=True)\n",
    "\n",
    "dataset = dataset_df.values.tolist()\n",
    "test_dataset = test_dataset_df.values.tolist()\n",
    "best_response = best_response_df.values.tolist()\n",
    "best_response_input = best_response_input_df.values.tolist()\n",
    "n_responses = n_responses_df.values.tolist()\n",
    "\n",
    "\n",
    "for i in range(runs):\n",
    "\n",
    "    test_match_data = test_match(test_dataset)\n",
    "    test_match_dataset_df = pd.concat([test_match_dataset_df, test_match_data])\n",
    "\n",
    "    same_best_response = True\n",
    "    for i in range(3,len(test_match_data[0]),2):\n",
    "        if test_match_data[0][i][\"correct\"] == False:\n",
    "            sampled = test_match_data[0][i][\"sampled\"]\n",
    "            if sampled not in dataset_df[0].to_list():\n",
    "                content = test_match_data[0][i-1][\"prompt\"][1][\"content\"]\n",
    "                dataset.append([sampled,content,[]])\n",
    "                dataset_df = pd.DataFrame(dataset)\n",
    "                n = append_response(content, sampled, dataset)\n",
    "                n_responses.append([n])\n",
    "\n",
    "                data = battle(system_input,content,sampled,system_input,best_response_input[-1][0],best_response[-1][0]) \n",
    "                battles_df = pd.concat([battles_df,pd.DataFrame(data, index=[0])], ignore_index=True)\n",
    "\n",
    "                if data[\"Choice\"] == \"Yes\":\n",
    "                    best_response_input.append([content])\n",
    "                    best_response.append([sampled])\n",
    "                    test_dataset = [content_to_list(content,dataset)]\n",
    "                    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "                    same_best_response = False\n",
    "\n",
    "    if same_best_response:\n",
    "        test_dataset0 = test_dataset.copy()\n",
    "        for t in test_dataset0:\n",
    "            if t[0] != \"\":\n",
    "                if t[1] not in test_dataset_df[0].to_list():\n",
    "                    test_dataset.append(content_to_list(t[1],dataset))\n",
    "                    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "            l = content_to_list(t[0], dataset)\n",
    "            for response in l[2]:\n",
    "                if response not in test_dataset_df[0].to_list():\n",
    "                    test_dataset.append(content_to_list(response,dataset))\n",
    "                    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "    best_response_df = pd.DataFrame(best_response)\n",
    "    best_response_input_df = pd.DataFrame(best_response_input)\n",
    "    n_responses_df = pd.DataFrame(n_responses)  \n",
    "    \n",
    "    dataset_df.to_json(os.path.join(dataset_path, \"dataset\"), lines=True, orient=\"records\")\n",
    "    test_dataset_df.to_json(os.path.join(dataset_path, \"test_dataset\"), lines=True, orient=\"records\")\n",
    "    best_response_df.to_json(os.path.join(dataset_path, \"best_response\"), lines=True, orient=\"records\")\n",
    "    best_response_input_df.to_json(os.path.join(dataset_path, \"best_response_input\"), lines=True, orient=\"records\")\n",
    "    n_responses_df.to_json(os.path.join(dataset_path, \"n_responses\"), lines=True, orient=\"records\")\n",
    "\n",
    "    pd.DataFrame(dataset_df[0]).to_json(os.path.join(dataset_path, \"dataset0\"), lines=True, orient=\"records\")\n",
    "    pd.DataFrame(dataset_df[1]).to_json(os.path.join(dataset_path, \"dataset1\"), lines=True, orient=\"records\")\n",
    "\n",
    "    battles_df.to_json(os.path.join(dataset_path, \"battles\"), lines=True, orient=\"records\")\n",
    "    test_match_dataset_df.to_json(os.path.join(dataset_path, \"test_match_dataset\"), lines=True, orient=\"records\")\n",
    "\n",
    "    responses = []\n",
    "    for responses_list in dataset_df[2]:\n",
    "        responses.append(len(responses_list))\n",
    "        pd.Series(responses)\n",
    "    responses_df = pd.DataFrame({\"len\":responses, \"responses\": dataset_df[2]})\n",
    "    responses_df.to_json(os.path.join(dataset_path, \"responses\"), lines=True, orient=\"records\")\n",
    "\n",
    "    best_responses_battles_df = battles_df[battles_df[\"Choice\"]==\"Yes\"]\n",
    "    pd.DataFrame(best_responses_battles_df[\"Sampled\"]).to_json(os.path.join(dataset_path, \"best_responses_battles_sampled\"), lines=True, orient=\"records\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
