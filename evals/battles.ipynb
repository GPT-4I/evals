{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1833/1397368638.py:98: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  battles_df = pd.read_json(os.path.join(dataset_path, \"battles\"), lines=True)\n",
      "/tmp/ipykernel_1833/1397368638.py:98: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  battles_df = pd.read_json(os.path.join(dataset_path, \"battles\"), lines=True)\n",
      "/tmp/ipykernel_1833/1397368638.py:98: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  battles_df = pd.read_json(os.path.join(dataset_path, \"battles\"), lines=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-28 09:34:05,605] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-28 09:34:06,331] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-28 09:34:06,332] [oaieval.py:138] \u001b[1;35mRun started: 230728093406R64KPFAI\u001b[0m\n",
      "[2023-07-28 09:34:06,334] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-28 09:34:06,334] [eval.py:33] Evaluating 8 samples\n",
      "[2023-07-28 09:34:06,338] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:02<00:00,  2.95it/s]\n",
      "[2023-07-28 09:34:09,070] [record.py:341] Final report: {'accuracy': 0.625, 'boostrap_std': 0.183983694929741}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-28 09:34:09,070] [oaieval.py:177] Final report:\n",
      "[2023-07-28 09:34:09,070] [oaieval.py:179] accuracy: 0.625\n",
      "[2023-07-28 09:34:09,070] [oaieval.py:179] boostrap_std: 0.183983694929741\n",
      "[2023-07-28 09:34:09,074] [record.py:330] Logged 16 rows of events to /workspaces/evals/evallogs/logs: insert_time=1.847ms\n",
      "[2023-07-28 09:34:10,390] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-28 09:34:11,117] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-28 09:34:11,118] [oaieval.py:138] \u001b[1;35mRun started: 230728093411O4IQDYFM\u001b[0m\n",
      "[2023-07-28 09:34:11,119] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-28 09:34:11,153] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-28 09:34:11,153] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-28 09:34:11,154] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-28 09:34:11,159] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.86s/it]\n",
      "[2023-07-28 09:34:17,026] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-28 09:34:17,027] [oaieval.py:177] Final report:\n",
      "[2023-07-28 09:34:17,027] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-28 09:34:17,027] [oaieval.py:179] score: 0.0\n",
      "[2023-07-28 09:34:17,028] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.492ms\n",
      "[2023-07-28 09:34:18,310] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-28 09:34:19,003] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-28 09:34:19,004] [oaieval.py:138] \u001b[1;35mRun started: 230728093419EDGC27R3\u001b[0m\n",
      "[2023-07-28 09:34:19,005] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-28 09:34:19,032] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-28 09:34:19,033] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-28 09:34:19,033] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-28 09:34:19,038] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.21s/it]\n",
      "[2023-07-28 09:34:25,255] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-28 09:34:25,255] [oaieval.py:177] Final report:\n",
      "[2023-07-28 09:34:25,255] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-28 09:34:25,255] [oaieval.py:179] score: 0.0\n",
      "[2023-07-28 09:34:25,257] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.824ms\n",
      "[2023-07-28 09:34:26,533] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-28 09:34:27,212] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-28 09:34:27,213] [oaieval.py:138] \u001b[1;35mRun started: 230728093427SMAGIES6\u001b[0m\n",
      "[2023-07-28 09:34:27,214] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-28 09:34:27,214] [eval.py:33] Evaluating 12 samples\n",
      "[2023-07-28 09:34:27,219] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|███████████████████████████████████████████| 12/12 [00:04<00:00,  2.64it/s]\n",
      "[2023-07-28 09:34:31,779] [record.py:341] Final report: {'accuracy': 0.6666666666666666, 'boostrap_std': 0.14454516787342137}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-28 09:34:31,779] [oaieval.py:177] Final report:\n",
      "[2023-07-28 09:34:31,779] [oaieval.py:179] accuracy: 0.6666666666666666\n",
      "[2023-07-28 09:34:31,779] [oaieval.py:179] boostrap_std: 0.14454516787342137\n",
      "[2023-07-28 09:34:31,782] [record.py:330] Logged 24 rows of events to /workspaces/evals/evallogs/logs: insert_time=2.236ms\n",
      "[2023-07-28 09:34:33,009] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-28 09:34:33,776] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-28 09:34:33,777] [oaieval.py:138] \u001b[1;35mRun started: 230728093433252SULVR\u001b[0m\n",
      "[2023-07-28 09:34:33,778] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-28 09:34:33,807] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-28 09:34:33,807] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-28 09:34:33,807] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-28 09:34:33,813] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.26s/it]\n",
      "[2023-07-28 09:34:40,080] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-28 09:34:40,080] [oaieval.py:177] Final report:\n",
      "[2023-07-28 09:34:40,080] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-28 09:34:40,080] [oaieval.py:179] score: 0.0\n",
      "[2023-07-28 09:34:40,081] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.612ms\n",
      "[2023-07-28 09:34:41,394] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-28 09:34:42,088] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-28 09:34:42,090] [oaieval.py:138] \u001b[1;35mRun started: 230728093442TK2QJQ24\u001b[0m\n",
      "[2023-07-28 09:34:42,091] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-28 09:34:42,118] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-28 09:34:42,119] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-28 09:34:42,119] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-28 09:34:42,125] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.75s/it]\n",
      "[2023-07-28 09:34:48,881] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-28 09:34:48,881] [oaieval.py:177] Final report:\n",
      "[2023-07-28 09:34:48,881] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-28 09:34:48,882] [oaieval.py:179] score: 0.0\n",
      "[2023-07-28 09:34:48,883] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.643ms\n",
      "[2023-07-28 09:34:50,179] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-28 09:34:51,139] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-28 09:34:51,140] [oaieval.py:138] \u001b[1;35mRun started: 230728093451OPSREG6H\u001b[0m\n",
      "[2023-07-28 09:34:51,141] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-28 09:34:51,142] [eval.py:33] Evaluating 17 samples\n",
      "[2023-07-28 09:34:51,146] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|███████████████████████████████████████████| 17/17 [00:05<00:00,  3.34it/s]\n",
      "[2023-07-28 09:34:56,270] [record.py:341] Final report: {'accuracy': 0.7058823529411765, 'boostrap_std': 0.12376893794486563}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-28 09:34:56,271] [oaieval.py:177] Final report:\n",
      "[2023-07-28 09:34:56,271] [oaieval.py:179] accuracy: 0.7058823529411765\n",
      "[2023-07-28 09:34:56,271] [oaieval.py:179] boostrap_std: 0.12376893794486563\n",
      "[2023-07-28 09:34:56,275] [record.py:330] Logged 34 rows of events to /workspaces/evals/evallogs/logs: insert_time=3.788ms\n",
      "[2023-07-28 09:34:57,494] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-28 09:34:58,216] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-28 09:34:58,217] [oaieval.py:138] \u001b[1;35mRun started: 230728093458CAW772MV\u001b[0m\n",
      "[2023-07-28 09:34:58,219] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-28 09:34:58,247] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-28 09:34:58,248] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-28 09:34:58,249] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-28 09:34:58,253] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "[2023-07-28 09:35:04,449] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-28 09:35:04,450] [oaieval.py:177] Final report:\n",
      "[2023-07-28 09:35:04,450] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-28 09:35:04,450] [oaieval.py:179] score: 0.0\n",
      "[2023-07-28 09:35:04,453] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.715ms\n",
      "[2023-07-28 09:35:05,693] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-28 09:35:06,479] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-28 09:35:06,480] [oaieval.py:138] \u001b[1;35mRun started: 230728093506CU3Y462A\u001b[0m\n",
      "[2023-07-28 09:35:06,482] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-28 09:35:06,513] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-28 09:35:06,513] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-28 09:35:06,514] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-28 09:35:06,519] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "[2023-07-28 09:35:12,633] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-28 09:35:12,633] [oaieval.py:177] Final report:\n",
      "[2023-07-28 09:35:12,633] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-28 09:35:12,633] [oaieval.py:179] score: 0.0\n",
      "[2023-07-28 09:35:12,634] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.550ms\n",
      "[2023-07-28 09:35:13,892] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-28 09:35:14,640] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-28 09:35:14,641] [oaieval.py:138] \u001b[1;35mRun started: 230728093514KVA5J7IQ\u001b[0m\n",
      "[2023-07-28 09:35:14,642] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-28 09:35:14,643] [eval.py:33] Evaluating 22 samples\n",
      "[2023-07-28 09:35:14,647] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|███████████████████████████████████████████| 22/22 [00:07<00:00,  3.05it/s]\n",
      "[2023-07-28 09:35:21,887] [record.py:341] Final report: {'accuracy': 0.6818181818181818, 'boostrap_std': 0.09971194048046687}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-28 09:35:21,887] [oaieval.py:177] Final report:\n",
      "[2023-07-28 09:35:21,887] [oaieval.py:179] accuracy: 0.6818181818181818\n",
      "[2023-07-28 09:35:21,887] [oaieval.py:179] boostrap_std: 0.09971194048046687\n",
      "[2023-07-28 09:35:21,892] [record.py:330] Logged 44 rows of events to /workspaces/evals/evallogs/logs: insert_time=3.819ms\n",
      "[2023-07-28 09:35:23,112] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-28 09:35:23,893] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-28 09:35:23,894] [oaieval.py:138] \u001b[1;35mRun started: 230728093523ZHMOSOQ4\u001b[0m\n",
      "[2023-07-28 09:35:23,896] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-28 09:35:23,923] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-28 09:35:23,923] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-28 09:35:23,923] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-28 09:35:23,929] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.35s/it]\n",
      "[2023-07-28 09:35:30,282] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-28 09:35:30,282] [oaieval.py:177] Final report:\n",
      "[2023-07-28 09:35:30,282] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-28 09:35:30,282] [oaieval.py:179] score: 0.0\n",
      "[2023-07-28 09:35:30,283] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.474ms\n",
      "[2023-07-28 09:35:31,650] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-28 09:35:32,381] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-28 09:35:32,383] [oaieval.py:138] \u001b[1;35mRun started: 230728093532HBKF6P42\u001b[0m\n",
      "[2023-07-28 09:35:32,384] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-28 09:35:32,412] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-28 09:35:32,412] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-28 09:35:32,413] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-28 09:35:32,417] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.67s/it]\n",
      "[2023-07-28 09:35:38,090] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-28 09:35:38,090] [oaieval.py:177] Final report:\n",
      "[2023-07-28 09:35:38,091] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-28 09:35:38,091] [oaieval.py:179] score: 0.0\n",
      "[2023-07-28 09:35:38,093] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.985ms\n",
      "[2023-07-28 09:35:39,398] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-28 09:35:40,076] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-28 09:35:40,078] [oaieval.py:138] \u001b[1;35mRun started: 230728093540WR6VJQ7A\u001b[0m\n",
      "[2023-07-28 09:35:40,079] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-28 09:35:40,107] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-28 09:35:40,107] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-28 09:35:40,108] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-28 09:35:40,113] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.20s/it]\n",
      "[2023-07-28 09:35:46,313] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-28 09:35:46,313] [oaieval.py:177] Final report:\n",
      "[2023-07-28 09:35:46,313] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-28 09:35:46,313] [oaieval.py:179] score: 0.0\n",
      "[2023-07-28 09:35:46,315] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.647ms\n"
     ]
    }
   ],
   "source": [
    "runs = 4\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"/workspaces/evals/dataset\"\n",
    "system_input = None\n",
    "\n",
    "def battle(system_input1, user_input1, completion1, system_input2, user_input2, completion2):\n",
    "\n",
    "    battle_path = os.path.join(os.getcwd(), \"registry\", \"data\", \"battle\")\n",
    "    os.makedirs(battle_path, exist_ok=True)\n",
    "    battle_path = os.path.join(battle_path, \"samples.jsonl\")\n",
    "\n",
    "    # Data\n",
    "    if system_input1 == None:\n",
    "        input1 = ([{\"role\":\"user\",\"content\":user_input1}])\n",
    "    else:\n",
    "        input1 = ([{\"role\":\"system\",\"content\":system_input1},{\"role\":\"user\",\"content\":user_input1}])\n",
    "\n",
    "    if system_input2 == None:\n",
    "        input2 = ([{\"role\":\"user\",\"content\":user_input2}])\n",
    "    else:\n",
    "        input2 = ([{\"role\":\"system\",\"content\":system_input2},{\"role\":\"user\",\"content\":user_input2}])\n",
    "\n",
    "    dataset = [{\"input1\": input1, \"completion1\": completion1, \"input2\": input2, \"completion2\":completion2}]\n",
    "    pd.DataFrame(dataset).to_json(battle_path, orient=\"records\", lines=True)\n",
    "    \n",
    "    # Run eval\n",
    "    !oaieval gpt-3.5-turbo battle --record_path /workspaces/evals/evallogs/logs\n",
    "    \n",
    "    # Report\n",
    "    with open(\"/workspaces/evals/evallogs/logs\", \"r\") as f:\n",
    "        events_df = pd.read_json(f, lines=True)\n",
    "\n",
    "    record_path = \"/workspaces/evals/evallogs/eval\"\n",
    "    os.makedirs(record_path, exist_ok=True)\n",
    "    events_df.to_json(os.path.join(record_path, \"events\"), lines=True, orient=\"records\")\n",
    "    events_df[\"data\"].to_json(os.path.join(record_path, \"data\"), lines=True, orient=\"records\")\n",
    "    data_df = pd.read_json(os.path.join(record_path, \"data\"), lines=True)\n",
    "    for row in data_df[0]:\n",
    "        s = str(row)\n",
    "        s = s.replace(\"[\",\"\",-1)\n",
    "        s = s.replace(\"]\",\"\",-1)\n",
    "        if s.startswith(\"{'choice':\"):\n",
    "            s = s.split(\"'\")[3]\n",
    "            choice = s\n",
    "        if s.startswith(\"{'prompt': {'role': 'user', 'content': 'You are comparing two responses to the following two instructions.\"):\n",
    "            sampled = s.split(\"\\\\n\\\\nReasoning:\\'}, \\'sampled\\': \")[1].replace(\"\\\\'\",\"'\").replace(\"\\\\n\",\"\\n\")\n",
    "    \n",
    "    data = {'Choice': choice, 'Instruction1': user_input1, 'Response1': completion1, 'Instruction2': user_input2, 'Response2': completion2, 'Sampled': sampled}\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def test_match(test_dataset):\n",
    "    \n",
    "    data = []\n",
    "    for t in range(0,len(test_dataset)):\n",
    "        data.append({\"input\":[{\"role\":\"system\",\"content\":\"\"},{\"role\":\"user\",\"content\":test_dataset[t][0]}], \"ideal\":test_dataset[t][2]})\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Paths. Assuming this notebook is in evals/evals\n",
    "\n",
    "    test_match_path = os.path.join(os.getcwd(), \"registry\", \"data\", \"match\")\n",
    "    os.makedirs(test_match_path, exist_ok=True)\n",
    "    test_match_path = os.path.join(test_match_path, \"samples.jsonl\")\n",
    "\n",
    "    df.to_json(test_match_path, orient=\"records\", lines=True)\n",
    "    \n",
    "    !oaieval gpt-3.5-turbo match --record_path /workspaces/evals/evallogs/logs\n",
    "    \n",
    "    events = \"/workspaces/evals/evallogs/logs\"\n",
    "    record_path = os.path.join(\"/workspaces/evals/evallogs/eval\")\n",
    "    with open(events, \"r\") as f:\n",
    "        events_df = pd.read_json(f, lines=True)\n",
    "    \n",
    "    os.makedirs(record_path, exist_ok=True)\n",
    "    events_df.to_json(os.path.join(record_path, \"events\"), lines=True, orient=\"records\")\n",
    "    events_df[\"data\"].to_json(os.path.join(record_path, \"data\"), lines=True, orient=\"records\")\n",
    "    data_df = pd.read_json(os.path.join(record_path, \"data\"), lines=True)\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "def content_to_list(content, dataset):\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    return dataset[dataset_df[0].to_list().index(content)]\n",
    "\n",
    "def append_response(content, response, dataset):\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    index = dataset_df[0].to_list().index(content)\n",
    "    dataset[index][2].append(response)\n",
    "    return len(dataset[index][2])\n",
    "dataset_df = pd.read_json(os.path.join(dataset_path, \"dataset\"), lines=True)\n",
    "test_dataset_df = pd.read_json(os.path.join(dataset_path, \"test_dataset\"), lines=True)\n",
    "best_response_df = pd.read_json(os.path.join(dataset_path, \"best_response\"), lines=True)\n",
    "best_response_input_df = pd.read_json(os.path.join(dataset_path, \"best_response_input\"), lines=True)\n",
    "battles_df = pd.read_json(os.path.join(dataset_path, \"battles\"), lines=True)\n",
    "n_responses_df = pd.read_json(os.path.join(dataset_path, \"n_responses\"), lines=True)\n",
    "test_match_dataset_df = pd.read_json(os.path.join(dataset_path, \"test_match_dataset\"), lines=True)\n",
    "\n",
    "dataset = dataset_df.values.tolist()\n",
    "test_dataset = test_dataset_df.values.tolist()\n",
    "best_response = best_response_df.values.tolist()\n",
    "best_response_input = best_response_input_df.values.tolist()\n",
    "n_responses = n_responses_df.values.tolist()\n",
    "\n",
    "\n",
    "for i in range(runs):\n",
    "\n",
    "    test_match_data = test_match(test_dataset)\n",
    "    test_match_dataset_df = pd.concat([test_match_dataset_df, test_match_data])\n",
    "\n",
    "    same_best_response = True\n",
    "    for i in range(3,len(test_match_data[0]),2):\n",
    "        if test_match_data[0][i][\"correct\"] == False:\n",
    "            sampled = test_match_data[0][i][\"sampled\"]\n",
    "            if sampled not in dataset_df[0].to_list():\n",
    "                content = test_match_data[0][i-1][\"prompt\"][1][\"content\"]\n",
    "                dataset.append([sampled,content,[]])\n",
    "                dataset_df = pd.DataFrame(dataset)\n",
    "                n = append_response(content, sampled, dataset)\n",
    "                n_responses.append([n])\n",
    "\n",
    "                data = battle(system_input,content,sampled,system_input,best_response_input[-1][0],best_response[-1][0]) \n",
    "                battles_df = pd.concat([battles_df,pd.DataFrame(data, index=[0])], ignore_index=True)\n",
    "\n",
    "                if data[\"Choice\"] == \"Yes\":\n",
    "                    best_response_input.append([content])\n",
    "                    best_response.append([sampled])\n",
    "                    test_dataset = [content_to_list(content,dataset)]\n",
    "                    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "                    same_best_response = False\n",
    "\n",
    "    if same_best_response:\n",
    "        test_dataset0 = test_dataset.copy()\n",
    "        for t in test_dataset0:\n",
    "            if t[0] != \"\":\n",
    "                if t[1] not in test_dataset_df[0].to_list():\n",
    "                    test_dataset.append(content_to_list(t[1],dataset))\n",
    "                    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "            l = content_to_list(t[0], dataset)\n",
    "            for response in l[2]:\n",
    "                if response not in test_dataset_df[0].to_list():\n",
    "                    test_dataset.append(content_to_list(response,dataset))\n",
    "                    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "    best_response_df = pd.DataFrame(best_response)\n",
    "    best_response_input_df = pd.DataFrame(best_response_input)\n",
    "    n_responses_df = pd.DataFrame(n_responses)  \n",
    "    \n",
    "    dataset_df.to_json(os.path.join(dataset_path, \"dataset\"), lines=True, orient=\"records\")\n",
    "    test_dataset_df.to_json(os.path.join(dataset_path, \"test_dataset\"), lines=True, orient=\"records\")\n",
    "    best_response_df.to_json(os.path.join(dataset_path, \"best_response\"), lines=True, orient=\"records\")\n",
    "    best_response_input_df.to_json(os.path.join(dataset_path, \"best_response_input\"), lines=True, orient=\"records\")\n",
    "    n_responses_df.to_json(os.path.join(dataset_path, \"n_responses\"), lines=True, orient=\"records\")\n",
    "\n",
    "    pd.DataFrame(dataset_df[0]).to_json(os.path.join(dataset_path, \"dataset0\"), lines=True, orient=\"records\")\n",
    "    pd.DataFrame(dataset_df[1]).to_json(os.path.join(dataset_path, \"dataset1\"), lines=True, orient=\"records\")\n",
    "\n",
    "    battles_df.to_json(os.path.join(dataset_path, \"battles\"), lines=True, orient=\"records\")\n",
    "    test_match_dataset_df.to_json(os.path.join(dataset_path, \"test_match_dataset\"), lines=True, orient=\"records\")\n",
    "\n",
    "    responses = []\n",
    "    for responses_list in dataset_df[2]:\n",
    "        responses.append(len(responses_list))\n",
    "        pd.Series(responses)\n",
    "    responses_df = pd.DataFrame({\"len\":responses, \"responses\": dataset_df[2]})\n",
    "    responses_df.to_json(os.path.join(dataset_path, \"responses\"), lines=True, orient=\"records\")\n",
    "\n",
    "    best_responses_battles_df = battles_df[battles_df[\"Choice\"]==\"Yes\"]\n",
    "    pd.DataFrame(best_responses_battles_df[\"Sampled\"]).to_json(os.path.join(dataset_path, \"best_responses_battles_sampled\"), lines=True, orient=\"records\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'prompt': [{'role': 'system', 'content': ''},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'correct': True, 'expected': 'Thank you so mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'prompt': [{'role': 'system', 'content': ''},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'correct': False, 'expected': [], 'picked': N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>{'correct': True, 'expected': ['Thank you so m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>{'prompt': [{'role': 'system', 'content': ''},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>{'correct': False, 'expected': [], 'picked': N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>{'prompt': [{'role': 'system', 'content': ''},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>{'correct': False, 'expected': [], 'picked': N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0                                                 None\n",
       "1    {'prompt': [{'role': 'system', 'content': ''},...\n",
       "2    {'correct': True, 'expected': 'Thank you so mu...\n",
       "3    {'prompt': [{'role': 'system', 'content': ''},...\n",
       "4    {'correct': False, 'expected': [], 'picked': N...\n",
       "..                                                 ...\n",
       "127  {'correct': True, 'expected': ['Thank you so m...\n",
       "128  {'prompt': [{'role': 'system', 'content': ''},...\n",
       "129  {'correct': False, 'expected': [], 'picked': N...\n",
       "130  {'prompt': [{'role': 'system', 'content': ''},...\n",
       "131  {'correct': False, 'expected': [], 'picked': N...\n",
       "\n",
       "[132 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_match_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
