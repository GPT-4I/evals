{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "#import openai\n",
    "\n",
    "# Paths. Assuming this notebook is in evals/evals\n",
    "\n",
    "registry_path = os.path.join(os.getcwd(), \"registry\")\n",
    "dataset_path = (\"/workspaces/evals/dataset\")\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "os.makedirs(\"/workspaces/evals/evallogs\", exist_ok=True)\n",
    "\n",
    "# Registry yaml\n",
    "\n",
    "registry_yaml = {}\n",
    "registry_yaml[\"battle\"] = {\n",
    "    \"id\": \"battle.test.v1\",\n",
    "    \"metrics\": [\"accuracy\"]\n",
    "}\n",
    "registry_yaml[\"battle.test.v1\"] = {\n",
    "    \"class\": \"evals.elsuite.modelgraded.classify:ModelBasedClassify\",\n",
    "    \"args\": {\n",
    "        \"samples_jsonl\": \"battle/samples.jsonl\",\n",
    "        \"eval_type\": \"cot_classify\",\n",
    "        \"modelgraded_spec\": \"battle\",\n",
    "        \"sample_kwargs\":\n",
    "            {\"temperature\": 0.0}  }\n",
    "}\n",
    "\n",
    "\n",
    "with open(os.path.join(registry_path, \"evals\", \"battle.yaml\"), \"w\") as f:\n",
    "    yaml.dump(registry_yaml, f)\n",
    "\n",
    "\n",
    "registry_yaml = {}\n",
    "registry_yaml[\"match\"] = {\n",
    "    \"id\": \"match.test.v1\",\n",
    "    \"metrics\": [\"accuracy\"]\n",
    "}\n",
    "registry_yaml[\"match.test.v1\"] = {\n",
    "    \"class\": \"evals.elsuite.basic.sample_match:Match\",\n",
    "    \"args\": {\n",
    "        \"samples_jsonl\": \"match/samples.jsonl\",\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "with open(os.path.join(registry_path, \"evals\", \"match.yaml\"), \"w\") as f:\n",
    "    yaml.dump(registry_yaml, f)\n",
    "\n",
    "\n",
    "# Definitions\n",
    "\n",
    "def battle(system_input1, user_input1, completion1, system_input2, user_input2, completion2):\n",
    "\n",
    "    # Data\n",
    "\n",
    "    input1 = ([{\"role\":\"system\",\"content\":system_input1},{\"role\":\"user\",\"content\":user_input1}])\n",
    "    input2 = ([{\"role\":\"system\",\"content\":system_input2},{\"role\":\"user\",\"content\":user_input2}])\n",
    "\n",
    "    dataset = [{\"input1\": input1, \"completion1\": completion1, \"input2\": input2, \"completion2\":completion2}]\n",
    "   \n",
    "    # Paths. Assuming this notebook is in evals/evals\n",
    "\n",
    "    battle_path = os.path.join(os.getcwd(), \"registry\", \"data\", \"battle\")\n",
    "    os.makedirs(battle_path, exist_ok=True)\n",
    "    battle_path = os.path.join(battle_path, \"samples.jsonl\")\n",
    "\n",
    "    df = pd.DataFrame(dataset)\n",
    "    df.to_json(battle_path, orient=\"records\", lines=True)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    message_content = \"You are comparing two responses to the following two instructions.\"\n",
    "    \n",
    "    message_content += \"\\n\\n[Instruction 1]\\nUser: \"\n",
    "    message_content += user_input1\n",
    "    message_content += \"\\n\\n[Response 1]\\n\"\n",
    "    message_content += completion1\n",
    "\n",
    "    message_content += \"\\n\\n[Instruction 2]\\nUser: \"\n",
    "    message_content += user_input2\n",
    "    message_content += \"\\n\\n[Response 2]\\n\"\n",
    "    message_content += completion2 \n",
    "\n",
    "    message_content += \"\\n\\n\\nIs the first response better than the second? You must provide one answer based on your subjective view.\\n\\nFirst, write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Then print only a single choice from \\\"Yes\\\" or \\\"No\\\" (without quotes or punctuation) on its own line corresponding to the correct answer. At the end, repeat just the answer by itself on a new line.\\n\\nReasoning:\"\n",
    "\n",
    "    openai.api_key = \"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\"\n",
    "    data = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "        {\"role\": \"user\", \"content\": message_content},\n",
    "        ]\n",
    "    )\n",
    "    \"\"\"\n",
    "    !oaieval gpt-3.5-turbo battle --record_path /workspaces/evals/evallogs/logs\n",
    "    \n",
    "    events = \"/workspaces/evals/evallogs/logs\"\n",
    "    record_path = os.path.join(\"/workspaces/evals/evallogs/eval\")\n",
    "    with open(events, \"r\") as f:\n",
    "        events_df = pd.read_json(f, lines=True)\n",
    "\n",
    "    os.makedirs(record_path, exist_ok=True)\n",
    "    events_df.to_json(os.path.join(record_path, \"events\"), lines=True, orient=\"records\")\n",
    "    events_df[\"data\"].to_json(os.path.join(record_path, \"data\"), lines=True, orient=\"records\")\n",
    "    data_df = pd.read_json(os.path.join(record_path, \"data\"), lines=True)\n",
    "    for row in data_df[0]:\n",
    "        s = str(row)\n",
    "        s = s.replace(\"[\",\"\",-1)\n",
    "        s = s.replace(\"]\",\"\",-1)\n",
    "        if s.startswith(\"{'choice':\"):\n",
    "            s = s.split(\"'\")[3]\n",
    "            choice = s\n",
    "        if s.startswith(\"{'prompt': {'role': 'user', 'content': 'You are comparing two responses to the following two instructions.\"):\n",
    "            #s = s.split(\"\\\\n\\\\nInstruction 1\\\\n\",)[1]\n",
    "            #instruction1 = s.split(\"\\\\n\\\\nResponse 1\\\\n\")[0].replace(\"\\\\'\",\"'\").replace(\"\\\\n\",\"\\n\")\n",
    "            #s = s.split(\"\\\\nResponse 1\\\\n\",)[1]\n",
    "            #response1 = s.split(\"\\\\n\\\\nInstruction 2\\\\n\")[0].replace(\"\\\\'\",\"'\").replace(\"\\\\n\",\"\\n\")\n",
    "            #s = s.split(\"\\\\n\\\\nInstruction 2\\\\n\",)[1]\n",
    "            #instruction2 = s.split(\"\\\\n\\\\nResponse 2\\\\n\")[0].replace(\"\\\\'\",\"'\").replace(\"\\\\n\",\"\\n\")\n",
    "            #s = s.split(\"\\\\nResponse 2\\\\n\",)[1]\n",
    "            #response2 = s.split(\"\\\\n\\\\n\\\\nWhich of the following responses is better?\")[0].replace(\"\\\\'\",\"'\").replace(\"\\\\n\",\"\\n\")\n",
    "            sampled = s.split(\"\\\\n\\\\nReasoning:\\'}, \\'sampled\\': \")[1].replace(\"\\\\'\",\"'\").replace(\"\\\\n\",\"\\n\")\n",
    "    \n",
    "    #content = data[\"choices\"][\"message\"][\"content\"]\n",
    "    #choice = content.split(\"\\\\n\")[-1]\n",
    "    data = {'Choice': choice, 'Instruction1': user_input1, 'Response1': completion1, 'Instruction2': user_input2, 'Response2': completion2, 'Sampled': sampled}\n",
    "    \n",
    "    return data\n",
    "def test_match(test_dataset):\n",
    "    data = []\n",
    "    for t in range(0,len(test_dataset)):\n",
    "        data.append({\"input\":[{\"role\":\"system\",\"content\":\"\"},{\"role\":\"user\",\"content\":test_dataset[t][0]}], \"ideal\":test_dataset[t][2]})\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Paths. Assuming this notebook is in evals/evals\n",
    "\n",
    "    test_match_path = os.path.join(os.getcwd(), \"registry\", \"data\", \"match\")\n",
    "    os.makedirs(test_match_path, exist_ok=True)\n",
    "    test_match_path = os.path.join(test_match_path, \"samples.jsonl\")\n",
    "\n",
    "    df.to_json(test_match_path, orient=\"records\", lines=True)\n",
    "    \n",
    "    !oaieval gpt-3.5-turbo match --record_path /workspaces/evals/evallogs/logs\n",
    "    \n",
    "    events = \"/workspaces/evals/evallogs/logs\"\n",
    "    record_path = os.path.join(\"/workspaces/evals/evallogs/eval\")\n",
    "    with open(events, \"r\") as f:\n",
    "        events_df = pd.read_json(f, lines=True)\n",
    "    \n",
    "    os.makedirs(record_path, exist_ok=True)\n",
    "    events_df.to_json(os.path.join(record_path, \"events\"), lines=True, orient=\"records\")\n",
    "    events_df[\"data\"].to_json(os.path.join(record_path, \"data\"), lines=True, orient=\"records\")\n",
    "    data_df = pd.read_json(os.path.join(record_path, \"data\"), lines=True)\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "def content_to_list(content, dataset):\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    return dataset[dataset_df[0].to_list().index(content)]\n",
    "\n",
    "def append_response(content, response, dataset):\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    index = dataset_df[0].to_list().index(content)\n",
    "    dataset[index][2].append(response)\n",
    "    return len(dataset[index][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-25 10:11:44,240] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-25 10:11:45,017] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-25 10:11:45,018] [oaieval.py:138] \u001b[1;35mRun started: 230725101145IIKY7AYG\u001b[0m\n",
      "[2023-07-25 10:11:45,020] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-25 10:11:45,020] [eval.py:33] Evaluating 3 samples\n",
      "[2023-07-25 10:11:45,024] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  1.61it/s]\n",
      "[2023-07-25 10:11:46,906] [record.py:341] Final report: {'accuracy': 0.6666666666666666, 'boostrap_std': 0.4766539625346673}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-25 10:11:46,906] [oaieval.py:177] Final report:\n",
      "[2023-07-25 10:11:46,906] [oaieval.py:179] accuracy: 0.6666666666666666\n",
      "[2023-07-25 10:11:46,906] [oaieval.py:179] boostrap_std: 0.4766539625346673\n",
      "[2023-07-25 10:11:46,912] [record.py:330] Logged 6 rows of events to /workspaces/evals/evallogs/logs: insert_time=1.720ms\n",
      "[2023-07-25 10:11:48,136] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-25 10:11:49,087] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-25 10:11:49,089] [oaieval.py:138] \u001b[1;35mRun started: 230725101149FAXZES7J\u001b[0m\n",
      "[2023-07-25 10:11:49,090] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-25 10:11:49,117] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-25 10:11:49,117] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-25 10:11:49,117] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-25 10:11:49,125] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.72s/it]\n",
      "[2023-07-25 10:11:53,848] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-25 10:11:53,848] [oaieval.py:177] Final report:\n",
      "[2023-07-25 10:11:53,848] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-25 10:11:53,848] [oaieval.py:179] score: 0.0\n",
      "[2023-07-25 10:11:53,849] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.536ms\n",
      "[2023-07-25 10:11:55,092] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-25 10:11:55,813] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-25 10:11:55,815] [oaieval.py:138] \u001b[1;35mRun started: 230725101155TL7NSYIJ\u001b[0m\n",
      "[2023-07-25 10:11:55,816] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-25 10:11:55,816] [eval.py:33] Evaluating 4 samples\n",
      "[2023-07-25 10:11:55,821] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:02<00:00,  1.84it/s]\n",
      "[2023-07-25 10:11:58,016] [record.py:341] Final report: {'accuracy': 0.5, 'boostrap_std': 0.28920408019251737}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-25 10:11:58,016] [oaieval.py:177] Final report:\n",
      "[2023-07-25 10:11:58,016] [oaieval.py:179] accuracy: 0.5\n",
      "[2023-07-25 10:11:58,016] [oaieval.py:179] boostrap_std: 0.28920408019251737\n",
      "[2023-07-25 10:11:58,018] [record.py:330] Logged 8 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.883ms\n",
      "[2023-07-25 10:11:59,235] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-25 10:11:59,960] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-25 10:11:59,961] [oaieval.py:138] \u001b[1;35mRun started: 230725101159AYKFRT52\u001b[0m\n",
      "[2023-07-25 10:11:59,963] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-25 10:11:59,991] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-25 10:11:59,991] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-25 10:11:59,992] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-25 10:11:59,997] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.83s/it]\n",
      "[2023-07-25 10:12:05,836] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-25 10:12:05,836] [oaieval.py:177] Final report:\n",
      "[2023-07-25 10:12:05,836] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-25 10:12:05,836] [oaieval.py:179] score: 0.0\n",
      "[2023-07-25 10:12:05,838] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.576ms\n",
      "[2023-07-25 10:12:07,036] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-25 10:12:08,044] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-25 10:12:08,046] [oaieval.py:138] \u001b[1;35mRun started: 230725101208SQMPW7YL\u001b[0m\n",
      "[2023-07-25 10:12:08,047] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-25 10:12:08,075] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-25 10:12:08,076] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-25 10:12:08,076] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-25 10:12:08,080] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.87s/it]\n",
      "[2023-07-25 10:12:12,959] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-25 10:12:12,959] [oaieval.py:177] Final report:\n",
      "[2023-07-25 10:12:12,959] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-25 10:12:12,959] [oaieval.py:179] score: 0.0\n",
      "[2023-07-25 10:12:12,961] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.801ms\n",
      "[2023-07-25 10:12:14,212] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-25 10:12:14,921] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-25 10:12:14,923] [oaieval.py:138] \u001b[1;35mRun started: 230725101214PBLFB2IT\u001b[0m\n",
      "[2023-07-25 10:12:14,925] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-25 10:12:14,925] [eval.py:33] Evaluating 6 samples\n",
      "[2023-07-25 10:12:14,931] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:02<00:00,  2.93it/s]\n",
      "[2023-07-25 10:12:17,000] [record.py:341] Final report: {'accuracy': 0.6666666666666666, 'boostrap_std': 0.20654942911242014}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-25 10:12:17,000] [oaieval.py:177] Final report:\n",
      "[2023-07-25 10:12:17,000] [oaieval.py:179] accuracy: 0.6666666666666666\n",
      "[2023-07-25 10:12:17,000] [oaieval.py:179] boostrap_std: 0.20654942911242014\n",
      "[2023-07-25 10:12:17,003] [record.py:330] Logged 12 rows of events to /workspaces/evals/evallogs/logs: insert_time=1.325ms\n",
      "[2023-07-25 10:12:18,223] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-25 10:12:19,044] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-25 10:12:19,045] [oaieval.py:138] \u001b[1;35mRun started: 230725101219W4OPZECA\u001b[0m\n",
      "[2023-07-25 10:12:19,047] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-25 10:12:19,085] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-25 10:12:19,085] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-25 10:12:19,086] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-25 10:12:19,091] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.87s/it]\n",
      "[2023-07-25 10:12:22,968] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-25 10:12:22,969] [oaieval.py:177] Final report:\n",
      "[2023-07-25 10:12:22,969] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-25 10:12:22,969] [oaieval.py:179] score: 0.0\n",
      "[2023-07-25 10:12:22,970] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.511ms\n",
      "[2023-07-25 10:12:24,209] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-25 10:12:24,895] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-25 10:12:24,897] [oaieval.py:138] \u001b[1;35mRun started: 230725101224M4IWMGIG\u001b[0m\n",
      "[2023-07-25 10:12:24,898] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-25 10:12:24,928] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-25 10:12:24,928] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-25 10:12:24,928] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-25 10:12:24,934] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.53s/it]\n",
      "[2023-07-25 10:12:30,472] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-25 10:12:30,472] [oaieval.py:177] Final report:\n",
      "[2023-07-25 10:12:30,472] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-25 10:12:30,472] [oaieval.py:179] score: 0.0\n",
      "[2023-07-25 10:12:30,474] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.572ms\n",
      "[2023-07-25 10:12:31,729] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-25 10:12:32,545] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-25 10:12:32,546] [oaieval.py:138] \u001b[1;35mRun started: 230725101232NF2VMMWQ\u001b[0m\n",
      "[2023-07-25 10:12:32,548] [data.py:83] Fetching match/samples.jsonl\n",
      "[2023-07-25 10:12:32,548] [eval.py:33] Evaluating 7 samples\n",
      "[2023-07-25 10:12:32,552] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:01<00:00,  3.71it/s]\n",
      "[2023-07-25 10:12:34,462] [record.py:341] Final report: {'accuracy': 0.8571428571428571, 'boostrap_std': 0.16502525059315418}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-25 10:12:34,462] [oaieval.py:177] Final report:\n",
      "[2023-07-25 10:12:34,462] [oaieval.py:179] accuracy: 0.8571428571428571\n",
      "[2023-07-25 10:12:34,462] [oaieval.py:179] boostrap_std: 0.16502525059315418\n",
      "[2023-07-25 10:12:34,464] [record.py:330] Logged 14 rows of events to /workspaces/evals/evallogs/logs: insert_time=1.426ms\n",
      "[2023-07-25 10:12:35,717] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/evals\n",
      "[2023-07-25 10:12:36,494] [registry.py:270] Loading registry from /home/codespace/.evals/evals\n",
      "[2023-07-25 10:12:36,496] [oaieval.py:138] \u001b[1;35mRun started: 230725101236LZSJCL45\u001b[0m\n",
      "[2023-07-25 10:12:36,497] [registry.py:270] Loading registry from /workspaces/evals/evals/registry/modelgraded\n",
      "[2023-07-25 10:12:36,526] [registry.py:270] Loading registry from /home/codespace/.evals/modelgraded\n",
      "[2023-07-25 10:12:36,527] [data.py:83] Fetching battle/samples.jsonl\n",
      "[2023-07-25 10:12:36,527] [eval.py:33] Evaluating 1 samples\n",
      "[2023-07-25 10:12:36,532] [eval.py:139] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.47s/it]\n",
      "[2023-07-25 10:12:42,007] [record.py:341] Final report: {'counts/No': 1, 'score': 0.0}. Logged to /workspaces/evals/evallogs/logs\n",
      "[2023-07-25 10:12:42,007] [oaieval.py:177] Final report:\n",
      "[2023-07-25 10:12:42,007] [oaieval.py:179] counts/No: 1\n",
      "[2023-07-25 10:12:42,007] [oaieval.py:179] score: 0.0\n",
      "[2023-07-25 10:12:42,009] [record.py:330] Logged 3 rows of events to /workspaces/evals/evallogs/logs: insert_time=0.572ms\n"
     ]
    }
   ],
   "source": [
    "dataset_df = pd.read_json(os.path.join(dataset_path, \"dataset\"), lines=True)\n",
    "test_dataset_df = pd.read_json(os.path.join(dataset_path, \"test_dataset\"), lines=True)\n",
    "best_response_df = pd.read_json(os.path.join(dataset_path, \"best_response\"), lines=True)\n",
    "battles_df = pd.read_json(os.path.join(dataset_path, \"battles\"), lines=True)\n",
    "n_responses_df = pd.read_json(os.path.join(dataset_path, \"n_responses\"), lines=True)\n",
    "test_match_dataset_df = pd.read_json(os.path.join(dataset_path, \"test_match_dataset\"), lines=True)\n",
    "\n",
    "for i in range(4):\n",
    "\n",
    "    test_match_data = test_match(test_dataset)\n",
    "    test_match_dataset_df = pd.concat([test_match_dataset_df, test_match_data])\n",
    "\n",
    "    same_best_response = True\n",
    "    for i in range(3,len(test_match_data[0]),2):\n",
    "        if test_match_data[0][i][\"correct\"] == False:\n",
    "            content = test_match_data[0][i-1][\"prompt\"][1][\"content\"]\n",
    "            content_list = content_to_list(content,dataset)\n",
    "            if content_list[1] is not None:\n",
    "                parent_content_list = content_to_list(content_list[1],dataset)\n",
    "            sampled = test_match_data[0][i][\"sampled\"]\n",
    "            data = battle(system_input1,content,sampled,system_input2,best_response_input[-1],best_response[-1]) \n",
    "            dataset.append([sampled,content,[]])\n",
    "            battles.append(data)\n",
    "            n = append_response(content, sampled, dataset)\n",
    "            n_responses.append(n)\n",
    "            if data[\"Choice\"] == \"Yes\":\n",
    "                best_response_input.append(content)\n",
    "                best_response.append(sampled)\n",
    "                test_dataset = [content_list]\n",
    "                test_dataset_df = pd.DataFrame(test_dataset)\n",
    "                same_best_response = False\n",
    "\n",
    "    if same_best_response:\n",
    "        for t in test_dataset:\n",
    "            if t[1] is not None:\n",
    "                if t[1] not in test_dataset_df[0].to_list():\n",
    "                    test_dataset.append(content_to_list(t[1],dataset))\n",
    "                    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "            l = content_to_list(t[0], dataset)\n",
    "            for response in l[2]:\n",
    "                if response not in test_dataset_df[0].to_list():\n",
    "                    test_dataset.append(content_to_list(response,dataset))\n",
    "                    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "\n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    test_dataset_df = pd.DataFrame(test_dataset)\n",
    "    best_response_df = pd.DataFrame(best_response)\n",
    "    battles_df = pd.DataFrame(battles)\n",
    "    n_responses_df = pd.DataFrame(n_responses)\n",
    "\n",
    "    dataset_df.to_json(os.path.join(dataset_path, \"dataset\"), lines=True, orient=\"records\")\n",
    "    test_dataset_df.to_json(os.path.join(dataset_path, \"test_dataset\"), lines=True, orient=\"records\")\n",
    "    best_response_df.to_json(os.path.join(dataset_path, \"best_response\"), lines=True, orient=\"records\")\n",
    "    battles_df.to_json(os.path.join(dataset_path, \"battles\"), lines=True, orient=\"records\")\n",
    "    n_responses_df.to_json(os.path.join(dataset_path, \"n_responses\"), lines=True, orient=\"records\")\n",
    "    test_match_dataset_df.to_json(os.path.join(dataset_path, \"test_match_dataset\"), lines=True, orient=\"records\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  '',\n",
       "  [\"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\"]],\n",
       " ['',\n",
       "  None,\n",
       "  [\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\"]],\n",
       " [\"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\",\n",
       "  \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  []]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'prompt': [{'role': 'system', 'content': ''},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'correct': True, 'expected': 'Thank you! I ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'prompt': [{'role': 'system', 'content': ''},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'correct': True, 'expected': 'No problem! If ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'prompt': [{'role': 'system', 'content': ''},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'correct': False, 'expected': [], 'picked': N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'prompt': [{'role': 'system', 'content': ''},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'correct': True, 'expected': 'I'm sorry, I ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                                               None\n",
       "1                                               None\n",
       "2  {'prompt': [{'role': 'system', 'content': ''},...\n",
       "3  {'correct': True, 'expected': 'Thank you! I ap...\n",
       "4  {'prompt': [{'role': 'system', 'content': ''},...\n",
       "5  {'correct': True, 'expected': 'No problem! If ...\n",
       "6  {'prompt': [{'role': 'system', 'content': ''},...\n",
       "7  {'correct': False, 'expected': [], 'picked': N...\n",
       "8  {'prompt': [{'role': 'system', 'content': ''},...\n",
       "9  {'correct': True, 'expected': 'I'm sorry, I ca..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_match_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(test_match_data[0])-2)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " None,\n",
       " [\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate content on behalf of individuals.\",\n",
       "  \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\"]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df[0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       " \"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  '',\n",
       "  [\"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\"]],\n",
       " ['',\n",
       "  None,\n",
       "  [\"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\"]],\n",
       " [\"No problem! If you have any other questions or need assistance with something else, feel free to ask. I'm here to help!\",\n",
       "  \"I'm sorry, I cannot continue the text for you as I am an AI language model and I do not have access to personal information or the ability to generate text on behalf of someone else.\",\n",
       "  []]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'prompt': [{'role': 'system', 'content': ''},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'correct': False, 'expected': [], 'picked': N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                                               None\n",
       "1                                               None\n",
       "2  {'prompt': [{'role': 'system', 'content': ''},...\n",
       "3  {'correct': False, 'expected': [], 'picked': N..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_match_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=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\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "test_match() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(dataset_test)):\n\u001b[1;32m      3\u001b[0m     test_match_data \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m     test_match_data\u001b[39m.\u001b[39mappend(test_match(dataset_test[d][\u001b[39m0\u001b[39;49m],dataset_test[d][\u001b[39m2\u001b[39;49m]))\n\u001b[1;32m      5\u001b[0m dataset_test\u001b[39m.\u001b[39mappend(dataset(dataset[:][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mindex(dataset_test[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m])))\n",
      "\u001b[0;31mTypeError\u001b[0m: test_match() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "for d in range(0,len(dataset_test)):\n",
    "    test_match_data = []\n",
    "    test_match_data.append(test_match(dataset_test[d][0],dataset_test[d][2]))\n",
    "dataset_test.append(dataset(dataset[:][0].index(dataset_test[0][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.append([])\n",
    "for u in user_content:\n",
    "    data = battle(system_input1,u,system_input2,best_response_input)\n",
    "    if data[\"Choice\"] == \"Yes\":\n",
    "        best_response_input = data[\"Instruction1\"]\n",
    "        best_response.append(data[\"Response1\"]) \n",
    "    dataset[u].append(data)\n",
    "user_content.append(data[\"Response1\"])\n",
    "df = pd.DataFrame(dataset)\n",
    "user_content_df = pd.DataFrame(user_content)\n",
    "df.to_json(os.path.join(os.getcwd(), \"battles.jsonl\"), orient=\"records\", lines=True)\n",
    "user_content_df.to_json(os.path.join(os.getcwd(), \"battles_user_content.jsonl\"), orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-shot prompt\n",
    "\n",
    "choice = int(df[\"Choice\"][0])-1\n",
    "if choice == 1:\n",
    "    input[0] = [\n",
    "        {\"role\": \"system\", \"content\": user_input[choice], \"name\": \"example_user\"},\n",
    "        {\"role\": \"system\", \"content\": df[df.columns[1+(2*choice)]][0], \"name\": \"example_assistant\"},\n",
    "        {\"role\":\"system\",\"content\":\"\"},\n",
    "        {\"role\":\"user\",\"content\":user_input[1]},\n",
    "        ]\n",
    "    dataset = [{\"input1\": input[0], \"input2\": input[1]}]\n",
    "    df = pd.DataFrame(dataset)\n",
    "    df.to_json(os.path.join(os.getcwd(), \"battles.jsonl\"), orient=\"records\", lines=True)\n",
    "    df.to_json(os.path.join(os.getcwd(), \"battles.jsonl\"), orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instruction(i):\n",
    "    if i.startswith(\"User: \"):\n",
    "        input = [{\"role\":\"system\",\"content\":\"\"},{\"role\":\"user\",\"content\":i.removeprefix(\"User: \")}]\n",
    "    else:\n",
    "        ins = i.split(\"\\nUser: \")\n",
    "        input = [{\"role\":\"system\",\"content\":ins[0]},{\"role\":\"user\",\"content\":ins[1]}]\n",
    "    return input\n",
    "meta = []\n",
    "for index, row in df.iterrows():\n",
    "    meta.append({\"input1\": instruction(row[0]), \"completion1\": row[1], \"input2\": instruction(row[2]), \"completion2\": row[3], \"choice\":\"\"})\n",
    "meta = pd.DataFrame(meta)\n",
    "\n",
    "row = 5\n",
    "meta.iloc[row][0],meta.iloc[row][1],meta.iloc[row][2],meta.iloc[row][3]\n",
    "\n",
    "meta.iloc[row][4] = \"1\"\n",
    "#meta.to_json(meta_path, orient=\"records\", lines=True)\n",
    "!oaieval gpt-3.5-turbo dataset-meta --record_path /workspaces/evals/evallogs/logs-meta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
